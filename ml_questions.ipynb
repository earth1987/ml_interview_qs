{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4801859-1a4c-41c6-b7e6-4a602479700b",
   "metadata": {},
   "source": [
    "# Machine learning questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966dc7a-abd2-498e-a249-e1d3f71ab771",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1dccd-048a-4ee8-b4fe-e9b998d92362",
   "metadata": {},
   "source": [
    "### Framework\n",
    "\n",
    "Framework for approaching machine learning questions:\n",
    "\n",
    "        1) Define the scope\n",
    "        2) Establish model performance metrics\n",
    "        3) Choose data sources\n",
    "        4) Data exploration\n",
    "        5) Data cleansing\n",
    "        6) Feature engineering\n",
    "        7) Model selection\n",
    "        8) Model training & evaluation\n",
    "        9) Deployment\n",
    "        10) Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fcb4d-7158-4866-bfc4-84bec9baa572",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0908a-ce41-4cd3-8eec-2c0681da59bf",
   "metadata": {},
   "source": [
    "### Step 1: Define the scope\n",
    "\n",
    "***a. Context***\n",
    "\n",
    "* Confirm product/business goals the problem addresses\n",
    "* What stakeholders would be affected?\n",
    "* What are the implications of incorrect predictions?\n",
    "\n",
    "***b. Model***\n",
    "* What is the dependent variable we are trying to model?\n",
    "* Is ML needed? Would a rules based or human approach suffice?\n",
    "* What type of machine learning model (supervised/unsupervised)?\n",
    "* Is there a baseline we can compare performance against?\n",
    "* What is the desired improvement on any baseline?\n",
    "\n",
    "***c. Technical requirements***\n",
    "* What latency is needed?\n",
    "* Are there any prediction throughput requirements?\n",
    "* Where is the model being deployed? Does it need to fit on a device?\n",
    "  \n",
    "***d. Data requirements***\n",
    "* Are there legal/ethical constraints such as GDPR?\n",
    "* Is appropriate training data available? Is data collection required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bf801-2dce-4d92-a8ab-01d9d55b3256",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb1f3bb-3627-4190-ad85-88c5e60b91b5",
   "metadata": {},
   "source": [
    "### Step 2: Establish model performance metrics\n",
    "* Align model performance metrics with the business problem\n",
    "* A single metric makes it easier to rank model performance \n",
    "* Therefore choose a primary metric but discuss the trade-offs between different metrics (e.g. accuracy, precision, recall, F1-score)\n",
    "* It's OK to consider constraints for secondary metrics (e.g. optimise precision @ recall >=0.95)\n",
    "* Define success relative to the basline performance and desired improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711ecd4-0b8a-4821-aca6-07bc26cda6e4",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6a10a-78e6-4416-aa44-a261895e02db",
   "metadata": {},
   "source": [
    "### Step 3: Choose data sources\n",
    "\n",
    "***a. Articulate preferred data sources***\n",
    "* Internal company data\n",
    "* Online sources: HTML web scraping, APIs\n",
    "* Alternative data sources: crowdsourcing (Amazon Mechanical Turk), public, academic and 2nd/3rd party service providers\n",
    "\n",
    "***b. Assess data sources***\n",
    "* How was data collected? Is there any sampling, selection or response bias?\n",
    "* Is there a data dictionary?\n",
    "* How current is the data? When will it next be updated?\n",
    "\n",
    "***c. Address data source (and model) limitations***\n",
    "* Data augmentation\n",
    "* Artifical data synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5d493-d569-4823-aa19-d7cd36b3bd76",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990aa89a-b935-4d96-a566-ddfb9f8f1cab",
   "metadata": {},
   "source": [
    "### Step 4: Data exploration\n",
    "\n",
    "***a. Calculate summary statistics***\n",
    "* For continuous variables, calculate the mean, median and quantiles\n",
    "* For categorical variables, calculate the mode\n",
    "* Identify variables with low variance and therefore little predictive value\n",
    "* Count missing values\n",
    "\n",
    "***b. Visualise feature distributions***\n",
    "* For continuous variables, plot histograms and assess range, skewness, kurtosis and outliers.\n",
    "* For categorical variables, plot bar charts and assess cardinality.\n",
    "\n",
    "***c. Visualise relationships between variables***\n",
    "* Visualise bivariate relationships between features and the target using a matrix of scatter plots\n",
    "* Assess multi-collinearity with a correlation matrix\n",
    "\n",
    "***d. Check linearity***\n",
    "* Assess linearity using PCA or the matrix of scatter plots\n",
    "\n",
    "https://www.linkedin.com/pulse/quick-way-check-linearity-data-aditya-dutt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438e818-cd8a-40d2-bbff-9e1e407f11dd",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bce605-67a0-43c0-b958-8a39c2de3761",
   "metadata": {},
   "source": [
    "### Step 5: Clean your data\n",
    "\n",
    "***a. Erroneous data***\n",
    "* Identify differences in schema/formattingg\n",
    "* Identify data entry errors (incorrect values)\n",
    "* Identify duplicate rows\n",
    "\n",
    "***b. Missing values***\n",
    "* What features are missing data? Are missing values numerical or categorical?\n",
    "* Is it possible to use an additional data source to fill in the missing info?\n",
    "* Why is the data missing?\n",
    "    * *Missing completely at random (MCAR)*\n",
    "      - Randomly distributed across a given variable and the probability of a value being missing is unrelated to other variables.\n",
    "      - E.g. Skipping survey questions. Equipment malfunction. Data entry errors.\n",
    "        \n",
    "    * *Missing at random (MAR)*\n",
    "      - NOT randomly distributed as the probability of a value being missing is related to another variable.\n",
    "      - E.g. Systematic exclusions. Data not collected for a specific demographic.\n",
    "        \n",
    "    * *Not missing at random (NMAR)*\n",
    "       - Missing for reasons related to the values themselves.\n",
    "       - E.g. People not reporting their income **because** of the value. Fear of discrimination.\n",
    "\n",
    "***c. Outliers***\n",
    "\n",
    "The first step is to try and understand why outliers occurred. Different steps can then be followed.\n",
    "\n",
    "i. *Trimming*\n",
    "* If the points are truly anomalous, and not worth incorporating, they can be removed.\n",
    "* Risk losing information.\n",
    "\n",
    "ii. *Winsorization*\n",
    "* Cap the data at a threshold\n",
    "* A 90% winorization:\n",
    "    * Cap the bottom 5% of values at the 5th percentile\n",
    "    * Cap the top 5% of values at the 95th percentile\n",
    "\n",
    "\n",
    "Note: Addressing each of the above is time consuming. Where possible, build a baseline model using outliers and ommitting columns containing missing values, and records with erroneous data. If its performance is OK, there is no need to make amendments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63173581-73ce-4e2f-93d6-87bd7bd44abc",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb687173-626c-436d-9c82-9b2d9ab3b072",
   "metadata": {},
   "source": [
    "### Step 6: Feature engineering\n",
    "\n",
    "***a. Feature selection***\n",
    "* Select a relevant subset of features for model construction\n",
    "\n",
    "***b. Feature pre-processing - quantitative data***\n",
    "* *Binning* - used to reduce noise by breaking down a continuous variable into discrete bins\n",
    "* *Dimensionality reduction* (e.g. PCA) - used to generate a reduced set of uncorrelated features (negating multicollinearity & the curse of dimensionality)\n",
    "* *Transformations* (e.g. log, capping, flooring) - used when data is skewed or data must conform to a standard statistical distribution\n",
    "* *Feature scaling* (e.g. normalisation/standardisation) - used when the ML algorithm is distance based (and therefore sensitive to the variance of features e.g. K-means)\n",
    "\n",
    "Note: Standardisation via z-scores gives a mean of zero and a variance of one. Normalisation via min/max scaling rescales featurs from 0 to 1. Neither alter the shape of a distribution. The former is less sensitive to outliers (usually preferrable).\n",
    "\n",
    "***c. Feature pre-processing - categorical data***\n",
    "* *One-hot encoding* - used, when order doesn't matter (i.e. nominal), to create dummy variables for each category \n",
    "* *Label encoding* - used, when order does matter (i.e. ordinal), to convert a categorical feature into a discrete numerical feature \n",
    "* *Hashing* - used, when a nominal feature has high cardinality, to create a fixed subset of dummy variables\n",
    "\n",
    "Note: For categorical features with high cardinality, one hot encoding can trigger the curse of dimensionality. Hashing is appropriate in these instances.\n",
    "\n",
    "***d. Feature pre-processing - text data***\n",
    "* *Stemming* - reduces to the root word by removing the last few characters\n",
    "* *Lemmatization* - reduces to the root word by considering context\n",
    "* *Filtering* - remove \"stop words\" and punctuation\n",
    "* *Bag-of-words* - represents text as a collection of words by associating each word and its frequency\n",
    "* *N-grams* - represents text as a collection of sequences by associating sequences of N words with their frequency \n",
    "* *Word embeddings* - converts words to vectors that encode their meaning. Words closer in meaning are closer in vector space. Popular methods include word2vec and GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5e53b-0d0f-4386-9ecf-0337c9e55e56",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc223a18-c974-4602-aa65-9aee921df17f",
   "metadata": {},
   "source": [
    "### Step 7: Model selection\n",
    "\n",
    "<img src=\"figures/cheat_sheet.png\" align=\"center\" width=\"700\" />\n",
    "\n",
    "***a. What are you trying to achieve?***\n",
    "\n",
    "Unsupervised model\n",
    "* Are you trying to find hidden patterns in unlabeled data ({x1, x2, x3,...}) ?\n",
    "* Do you want to cluster data points or apply dimension reduction?\n",
    "\n",
    "Supervised model\n",
    "* Are you trying to predict a value y given labelled data ({(x1, y1), (x2, y2), (x3, y3), ...})?\n",
    "* Are you trying predict a numeric or categorical value? Is it a regression or classification problem?\n",
    "\n",
    "***b. Data***\n",
    "\n",
    "Number of records/features\n",
    "* How much training data is available?\n",
    "* How many features are present?\n",
    "* What is the ratio of features to records? Curse of dimensionality. Is dimension reduction required?\n",
    "\n",
    "Note: Some models perform poorly with little training data. Similarly, not every model scales well with the number of features. \n",
    "\n",
    "Data types\n",
    "* What data types are present? Is the data homogenous or heterogeneous?\n",
    "\n",
    "Linearity\n",
    "* Is a linear model required?\n",
    "\n",
    "Note: For regression problems, a linear model is appropriate if you can estimate the target using a line/plane/hyperplane after plotting all the features PLUS the outcome. For classification problems, a linear model is appropriate if there is (n-1) dimensional line/plane/hyperplane that seperates (or mostly seperates) different classes. Where n is the number of features.\n",
    "\n",
    "***c. Model performance and cost***\n",
    "\n",
    "Explainability\n",
    "* Does the model need to be explainable?\n",
    "\n",
    "Speed-accuracy tradeoff\n",
    "* Is accuracy more important than implementation time? Or vice-versa?\n",
    "* How fast does the model need to be at inference time?\n",
    "* Will the model need to be updated frequently?\n",
    "\n",
    "Note: Higher accuracy often means more complex models with more parameters and therefore longer training times. Furthermore, optimisation may be more difficult due to the risk of overfitting. If low implementation times are preferred, less complex models can be implemeneted.\n",
    "\n",
    "Financial budget\n",
    "* Is there a financial budget?\n",
    "\n",
    "Note: Training complex models can be expensive. Likewise regularly retraining models adds to cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fae7e2-8097-4920-a223-b6a41c60e628",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1ac15-67ff-4e45-ad33-81586871151e",
   "metadata": {},
   "source": [
    "### Step 8: Model training & evaluation\n",
    "\n",
    "***a. Train-validation-test split***\n",
    "\n",
    "*Supervised learning:*\n",
    "* We want to learn a function *h* that would predict y for a new pair (x, y)\n",
    "* Two problematic phenomenon when learning a model: 1) underfitting & 2) overfitting\n",
    "* Therefore split available data into three subsets: 1) training data (80%), 2) validation data (10%) and 3) test data(10%)\n",
    "* The validation set is used to check whether the function obtained during training is underfit/overfit\n",
    "* If loss on the validation set is too large, the model is revised\n",
    "\n",
    "*Unsupervised learning:*\n",
    "* We want to learn a function *h* to find hidden patterns in unlabelled data {(x1, y1), (x2, y2), (x3, y3)...}\n",
    "* We cannot validate performance using labels (e.g. via accuracy etc.), so no train-validation-test split is typically performed. There are some exceptions where data can be split:\n",
    "  \n",
    "        i. Dimensionality reduction techniques are usually tested by calculating the error in reconstruction\n",
    "        ii. Clustering can sometimes be validated by manually labelling test instances (or using pre-existing labels)\n",
    "\n",
    "\n",
    "***b. Cross validation***\n",
    "\n",
    "* A technique used to asses the performance of an algorithm\n",
    "* Motivation:\n",
    "1. Avoiding training & testing on the same subset of data, which would lead to overfitting\n",
    "2. Training results improve as training data increases in size but validation results become noisy as the validation set decreases in size. Avoiding using a dedicated validation set, with which no training can be done is therefore very useful for small datasets\n",
    "* The process is as follows:\n",
    "\n",
    "      i. Randomly shuffle data into *k* equally-sized blocks (folds)\n",
    "      ii. For each i in fold 1,2...k, train the model on all the data except for fold *i*. Evaluate the validation error using the ith fold.\n",
    "      iii. Average the *k* validation errors from step 2 to get an estimate of the generalisation error\n",
    "\n",
    "***c. Hyper-parameter tuning***\n",
    "* Hyper-parameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning\n",
    "* Hyper-parameters can be optimised by comparing different models using a grid search\n",
    "\n",
    "***d. Model evaluation metric***\n",
    "\n",
    "*Classification models:*\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* Specificity\n",
    "* F1 score\n",
    "\n",
    "*Regression models:*\n",
    "* Total sum of squares\n",
    "* Explained sum of squares\n",
    "* Residual sum of squares\n",
    "* Mallow's Cp\n",
    "* AIC\n",
    "* BIC\n",
    "* Adjusted R^2\n",
    "\n",
    "***e. Regularization***\n",
    "* Aims to reduce overfitting by penalising model complexity\n",
    "* Examples: L1, L2\n",
    "\n",
    "***f. Sampling techniques***\n",
    "\n",
    "*When training times are slow:*\n",
    "* Random sampling\n",
    "* Stratified sampling\n",
    "\n",
    "*When data is imbalanced:*\n",
    "* Undersampling majority class\n",
    "* Oversampling minority class\n",
    "* Generation of synthetic examples (e.g. SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea050e72-39bc-4836-afba-979fd5cacf46",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410ce7d-16b9-4d83-af4f-332cab3e4ebb",
   "metadata": {},
   "source": [
    "### Step 9: Deployment\n",
    "* Model deployment is the process of putting machine learning models into production\n",
    "* The process of operationalising the entire deployment processes is called MLOps. Popular tools include Airflow and MLFlow\n",
    "* The first question you need to answer is whether you should use batch inference or online inference to serve your models\n",
    "\n",
    "***a. Online inference (AKA realtime inference or dynamic inference)***\n",
    "\n",
    "<img src=\"figures/online_inference.png\" align=\"center\" width=\"300\" />\n",
    "\n",
    "* Predictions are generated in real time upon request (at any time of day)\n",
    "* Typically, these predictions are generated on a single observation of input data at runtime\n",
    "* Often exposed via a REST API\n",
    "\n",
    "*Advantages:*\n",
    "* Make predictions for any new data in real-time\n",
    "* Real time delivery of predictions enables use cases where predictions are required immediately\n",
    "\n",
    "*Disadvantages:*\n",
    "* Low latency requirements place constraints on infrastructure and model complexity (e.g. neural networks can be slow at inference due to the number of operations)\n",
    "* Online inference systems require robust monitoring solutions\n",
    "\n",
    "*Applications (realtime predictions required):*\n",
    "* Fraud detection\n",
    "* Automatic text completion\n",
    "* Speech recognition\n",
    "* Estimated time to delivery\n",
    "\n",
    "***b. Batch inference (AKA offline inference)***\n",
    "\n",
    "<img src=\"figures/batch_inference.png\" align=\"center\" width=\"300\" />\n",
    "\n",
    "\n",
    "* Predictions are generated for a batch of inputs (typically on a recurring schedule e.g. hourly) rather than processing each input data point individually in real time\n",
    "* Predictions are then stored in a database and can be made available to developers or end users\n",
    "\n",
    "*Advantages:*\n",
    "* scalable compute resources can be used (e.g. batch inference using parallelisation via Spark)\n",
    "* predictions generated during batch inference can be analyzed and post processed before being seen by stakeholders\n",
    "\n",
    "*Disadvantages:*\n",
    "* predictions are not available in real-time\n",
    "\n",
    "*Applications (large data volumes, low response time):*\n",
    "* Product recommendations for users (e.g. e-commerce and NetFlix) are batch generated daily, cached and retrieved when necessary. There is no need to generate reccomendations upon log-in.\n",
    "\n",
    "Note: Latency refers to the time it takes for a model to make a prediction, while throughput measures the number of predictions a model can make in a given time.\n",
    "\n",
    "***c. Model degradation***\n",
    "* Model performance should be monitored as it can decline over time\n",
    "\n",
    "*Training-serving skew:*\n",
    "\n",
    "A difference between model performance during training and performance during serving caused by:\n",
    "\n",
    "    i. A discrepancy between data handling in training and serving pipelines\n",
    "    ii. A change in the data between training and serving (e.g. new distributions)\n",
    "\n",
    "*Monitoring strategies*\n",
    "\n",
    "    i. Monitor a metric that correlates with accuracy (e.g. if a spam classifier generally predicts 30% of messages as spam but one day predicts 75%, it's likely the model is broken)\n",
    "    ii. Manually check 5% of all predictions, and monitor the accuracy of those\n",
    "    iii. Monitor retrospectively. If you make predictions about the future, then verify accuracy later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa26015-48c1-4adb-9fdd-7bb7f979ad19",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadbe20-d598-400c-957d-3971ad937dbc",
   "metadata": {},
   "source": [
    "### Step 10: Iterate\n",
    "Iterate on the design of deployed models via:\n",
    "\n",
    "* Monitoring changes in features, business objectives and evaluation metrics\n",
    "* Error analysis - look at bad predictions and group them based on the reason they occurred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8471f4-a2f0-428b-b942-9595af16ec65",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2305d75-8ff4-4262-ae5f-2ee3cea09e17",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c995765b-6a31-4665-b796-6d82fa5d2b88",
   "metadata": {},
   "source": [
    "## Easy questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fa724-e670-4bc1-b3ef-889cb8729076",
   "metadata": {},
   "source": [
    "#### Unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce21714-c213-45df-80d4-e2768e36ce86",
   "metadata": {},
   "source": [
    "**1. You are building a binary classifier for an unbalanced dataset (where 1 class is much rarer than the other, say 1% and 99%, respectively). How do you handle this situation?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996f3f0-d762-4cc5-8385-dcf113903a79",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Unbalanced classes can be dealt with in several ways.\n",
    "\n",
    "1. Can we get more data? Is the event inherently rare?\n",
    "\n",
    "2. Choose the appropriate performance metrics:\n",
    "   * Don't use accuracy\n",
    "   * Look at precision, recall, F1 score and the ROC curve\n",
    "     \n",
    "3. Apply resampling to the training data:\n",
    "   * Oversampling the minority class via bootstrapping\n",
    "   * Undersampling the majority class via bootstrapping\n",
    "     \n",
    "4) Generate synthetic examples for the training data\n",
    "   * SMOTE - creates synthetic examples of the minority class (random variations of instance attributes based on neighbours)\n",
    "     \n",
    "5) Ensemble models\n",
    "   * Apply boosting to reduce bias - higher weight is given to the minority class at each successive iteration\n",
    "     \n",
    "6) Design a custom cost function to penalise wrong classification of the rare class more than the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ec347-5ec2-4086-ac68-063e05ad059f",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13447c2-ef0e-4129-9e19-e5d40b57c810",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a3407-dee5-427b-8269-be3f05dcfa31",
   "metadata": {},
   "source": [
    "#### MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c411f-64b2-47fb-b842-8733a1d74630",
   "metadata": {},
   "source": [
    "**2. What are some differences you would expect in a model that minimises squared error versus a model that minimises absolute error? In which case would each error metric be appropriate?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7ee68-af62-4919-98db-7d204893f314",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "$$MAE = \\frac{1}{N} \\Sigma_i^N |y_i - y_{pred}|$$\n",
    "$$MSE = \\frac{1}{N} \\Sigma_i^N (y_i - y_{pred})^2$$\n",
    "<center> Where, N is the number of training samples </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579771e1-c7b9-434c-8a56-dded0add2c91",
   "metadata": {},
   "source": [
    "Key differences:\n",
    "* MSE is more sensitive to outliers as errors are squared before being averaged.\n",
    "* MSE is more efficient computationally as the gradient is easier to calculate during optimisation\n",
    "* Calculating the gradient of MAE requires linear programming (less efficient)\n",
    "\n",
    "Conclusion:\n",
    "* Use MAE if the model needs to be robust to outliers and computational efficiency isn't an issue (e.g. small training set)\n",
    "* Use MSE if the model doesn't need to be robust to outliers and computation time is an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdcdae8-30f6-41ae-ae76-a2be9ade911a",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a1dac-0f3d-4edb-916d-fdeb6226a17e",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c4859-51e7-43e1-b155-a6902e1f97c4",
   "metadata": {},
   "source": [
    "####  K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae621a1-30c8-4d8e-ae11-dff1daba0e36",
   "metadata": {},
   "source": [
    "**3. When performing K-means clustering, how do you choose K?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b92178-4d9c-4265-bf97-c4ada7d60ab6",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "There is no perfect method for picking k (otherwise it would be a supervised problem).\n",
    "1. ***Business intuition***\n",
    "* Do you expect a certain number of clusters?\n",
    "* Visualise features within expected groups, do they behave similarly?\n",
    "<br>\n",
    "<br>\n",
    "2. ***Elbow method***\n",
    "* A few clusters should explain a lot of the variation in data \n",
    "* Plot the Within-Cluster-Sum of Squared Errors (WSS) for different values of k - at what point are there diminishing returns?\n",
    "* Calculation for each k:\n",
    "  * Fit k-mean clustering model\n",
    "  * Calculate the Squared Error for each point from the centroid of its cluster\n",
    "  * Sum the squared error across all points giving WSS\n",
    "  * Plot WSS versus k and choose k for which WSS becomes first starts to diminish\n",
    "<br>\n",
    "<br>\n",
    "3. ***Silhouette method***\n",
    "$$ Silhouette score = \\frac{(x-y)}{max(x,y)}$$\n",
    "<center> Where, x = mean distance to points of the nearest cluster & y = mean distance to points in the same cluster. Euclidian distance usually used. </center>\n",
    "* A silhouette score measures how similar a point is to its own cluster (cohesion) compared to other clusters (seperation)\n",
    "* The score ranges from -1 to + 1. A high value indicates a point is placed in the correct cluster\n",
    "* Calculation for each k:\n",
    "    * Calculate a silhouette for each point\n",
    "    * Plot a clustered bar chart showing scores for each point in their respective cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b95c1-381f-4d93-bede-f7e160c99ea2",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23c1a7-a6e4-469d-9c6f-8eb7e9dbc436",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d9fbf-27d4-4f96-8306-198d93d9af1c",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530f04d-2a33-495a-b67f-eab146b86b66",
   "metadata": {},
   "source": [
    "**4. How can you make you models more robust to outliers?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2551bb-0cee-43d7-88d3-b5b5939c1b3e",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "The first step is to try and understand why outliers occurred. Different steps can then be followed.\n",
    "1. ***Trimming***\n",
    "* If the points are truly anomalous, and not worth incorporating, they can be removed.\n",
    "* Risk losing information.\n",
    "<br>\n",
    "2. ***Winsorization***\n",
    "* Cap the data at a threshold\n",
    "* A 90% winorization:\n",
    "    * Cap the bottom 5% of values at the 5th percentile\n",
    "    * Cap the top 5% of values at the 95th percentile\n",
    "<br>\n",
    "3. ***Change the cost function***\n",
    "* The mean absolute error cost function is more robust to outliers than the mean squared error cost function (see above)\n",
    "<br>\n",
    "4. ***Add regularization***\n",
    "* L1 & L2 regularization reduce variance by minimising model weights\n",
    "<br>\n",
    "5. ***Transform the data***\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c04f3-e048-4c22-8322-ef946a8d19ee",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66d2f-320e-422f-ab71-e642e7034546",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982995ee-c019-4102-9edd-4ed772128525",
   "metadata": {},
   "source": [
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6be7a6-7b9c-4805-b78c-6c3ce3e2398f",
   "metadata": {},
   "source": [
    "**5. Say that you are running a multiple linear regression and that you have reason to believe that several of the predictors are correlated. How will the results behave if several are indeed correlated? How would you deal with this problem?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca319a26-ccbf-42f0-93c6-cace13e9395d",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Two primary problems relate to uncertainty of feature importance\n",
    "1. ***P values are misleading***\n",
    "* Important variables may have higher, statistically insignificant, P-values (as importance split over correlated variables)\n",
    "<br>\n",
    "\n",
    "2. ***Coefficient estimates are unstable***\n",
    "* Coefficients vary depending on which variables included\n",
    "* Imprecise estimates of coefficients lead to broad confidence intervals (maybe including zero)\n",
    "<br>\n",
    "\n",
    "Solutions:\n",
    "1. ***Remove correlated predictors***\n",
    "* Remove variables clearly related to the other (e.g. X and 2X)\n",
    "* Use a latent (i.e. hidden) variable relating to correlated variables (e.g. speed replaces distance & time)\n",
    "<br>\n",
    "\n",
    "2. ***Combine correlated predictors***\n",
    "* Combine collerated variables using PCA\n",
    "* Calculate interaction terms - e.g product of the two that are correlated\n",
    "<br>\n",
    "\n",
    "3. ***Regularization***\n",
    "* Use L2 regularization (e.g. Ridge regression) to stabilise the size of the coefficients\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c5cd8-1b1a-404f-b7e3-729dd9e5b0a9",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf21b26-1812-4267-a9a4-3d0cf8318a2e",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d72392-6907-49f1-b309-4db43b494d71",
   "metadata": {},
   "source": [
    "#### Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6eda5-6728-41a2-84bf-b4e470ffb35e",
   "metadata": {},
   "source": [
    "**6. Describe the motivation behind random forests. What are two ways in which they improve upon individual decision trees?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fee0c3-c7c5-4de8-a54c-61eabe071663",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Decision trees are prone to overfitting. Random forests are a type of ensemble learning.\n",
    "1. They reduce overfitting and therefore variance via bagging (bootstrap aggregating).\n",
    "2. Each consitituent decision tree is trained on a random subsample of the predictor variables. This decorrelates the trees meaning they are not equivalent and learn about other features of the data. Without it they would all prioritise the strong predictors.\n",
    "3. Random forests can be used to produce feature importance values [(see here)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html). \n",
    "4. Easy to implement and fast to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9ef71-5080-44eb-a718-3632d64b3392",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694c033-161b-41a0-9685-f0a36fea27f8",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aad759-e848-4108-a7b3-20f43b2fb4d3",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a525e2-31b7-4ec8-aab0-470461c000e5",
   "metadata": {},
   "source": [
    "**7. Given a large dataset of payment transactions, say we want to predict the likelihood of a given transaction being fraudulent. However, there are many rows with missing values for various columns. How would you deal with this?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bb012-d7b7-41ff-98f3-d2863fc55725",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Approach in a series of steps:\n",
    "\n",
    "1. ***Characterise***\n",
    "* What features are missing data? Are missing values numerical or categorical?\n",
    "* Is it possible to use an additional data source to fill in the missing info?\n",
    "* Why is the data missing?\n",
    "    * *Missing completely at random (MCAR)*\n",
    "      - Randomly distributed across a given variable and the probability of a value being missing is unrelated to other variables.\n",
    "      - E.g. Skipping survey questions. Equipment malfunction. Data entry errors.\n",
    "        \n",
    "    * *Missing at random (MAR)*\n",
    "      - NOT randomly distributed as the probability of a value being missing is related to another variable.\n",
    "      - E.g. Systematic exclusions. Data not collected for a specific demographic.\n",
    "        \n",
    "    * *Not missing at random (NMAR)*\n",
    "       - Missing for reasons related to the values themselves.\n",
    "       - E.g. People not reporting their income **because** of the value. Fear of discrimination.\n",
    "\n",
    "2. ***Establish a baseline***\n",
    "* Build a baseline model - does it meet business goals?\n",
    "* Is the missing data problem?\n",
    "  * MCAR - Do the relevant features have predictive value?\n",
    "  * MAR - Is the missing data within a category where fraudulent transactions never occur?\n",
    "\n",
    "4. ***Impute missing data***\n",
    "* If the baseline model is not OK - impute!\n",
    "  * Mean/median value (simple but dosn't factor in other features and correlations)\n",
    "  * Use a nearest neighbour method to estimate a value based on other features\n",
    "    \n",
    "6. ***Check performance with imputed data***\n",
    "* Use cross validation to compare performance of model with/without imputed data. If there is no change - alter or remove missing data.\n",
    "\n",
    "Note: A performance increase would only be expected if the imputed features have predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17f481-98b3-4f2f-af71-83ec74cdd55e",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71c0b6-a4a5-4394-882b-f446931cbc51",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c05cb-a1bc-4ae5-b26d-cb7ec7317e9a",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e220b-9289-4ddf-878f-cae286764089",
   "metadata": {},
   "source": [
    "**8. Say you are running a simple logistic regression to solve a problem but find the results to be unsatisfactiory. What are some ways you might improve your model, or what other models might you look into using instead?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65653c3-2aae-4ca3-946e-eb03b343c3c6",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "***Model improvements***\n",
    "* Logistic regression models often have high bias - add more features\n",
    "* Make sure all features are normalised (so they don't dominate model performance)\n",
    "* Perform feature selection - removing features with no predictive value may reduce noise\n",
    "* Perform k-fold cross validation to optimise hyperparameters: e.g. choose a form of regularization to reduce overfitting\n",
    "\n",
    "***Alternative models***\n",
    "* Logistic regression provides a linear decision boundary.\n",
    "* The classes may not be linearly seperable, therefore try other classification methods:\n",
    "    * Support vector machine (SVM)\n",
    "    * Tree-based approaches\n",
    "    * Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f0b9c-a7ea-4946-a688-4272a9a46b7a",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888f672-2803-4584-b9ba-df11a863c8a4",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24afe98-ef77-4a62-aab1-b3e9dc48b5c4",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844e98d-063a-4ef4-9cf7-bc79ef8c25bd",
   "metadata": {},
   "source": [
    "**9. Say you were running a linear regression for a dataset but you accidentally duplicated every data point. What happens to your beta coefficient?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7031b-442e-4d0c-a215-647234572c21",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06fddbf-2cc1-4ec1-a564-7e783e9843ae",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e46d6-2faf-4009-b689-88c0bc42ce6c",
   "metadata": {},
   "source": [
    "#### Boosting & bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b26c08-081f-47e2-9426-3df30e780482",
   "metadata": {},
   "source": [
    "**10. Compare and contrast gradient boosting and random forests.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a9a5a-69a0-4c2c-9810-d31b96eac172",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Both are forms of ensemble learning. Key differences:\n",
    "\n",
    "***Training***\n",
    "* Random forests rely on independant parallel training of decision trees using bootstrap aggregating (bagging)\n",
    "* Gradient boosting relies on sequential training of models where weak learners learn from the mistakes of preceding weak learners\n",
    "\n",
    "***Testing***\n",
    "$$\\hat{f}(x)= mode\\{\\hat{f}_1(x),\\hat{f}_2(x),1,...,\\hat{f}_m(x)\\}$$\n",
    "$$\\hat{f}(x)=\\frac{1}{M} \\sum_{m=1}^M\\hat{f}_m(x)$$\n",
    "<center> In random forests, the output of the trees is combined at test time via averaging or majority voting </center>\n",
    "\n",
    "$$\\hat{f}(x)=\\sum_{b=1}^B \\lambda \\hat{f}_b(x)$$\n",
    "<center> In boosting, models are combined sequentialy during training using a weighting. A final model is then applied at test time. </center>\n",
    "\n",
    "***Characteristics***\n",
    "* Gradient boosting is more prone to overfitting due to lack of independence and focus on mistakes\n",
    "* Gradient boosting hyperparameters are harder to tune\n",
    "* Gradient boosting can take longer to train overall due to sequential training of constituent models\n",
    "\n",
    "***Applications***\n",
    "* Gradient boosting is better for unbalanced datasets\n",
    "* Random forests are better for multi-class object detection with noisy data (e.g. computer vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f1d3c-9a14-4659-9a2f-8d52e41f62af",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103865b-8121-4e56-9143-39afbab23b5f",
   "metadata": {},
   "source": [
    "#### Estimate time of arrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937819fa-c571-4467-aa3f-6c752e1c4752",
   "metadata": {},
   "source": [
    "**11. Say that DoorDash is launching in Singapore. For this new market, you want to predict the estimated time of arrival (ETA) for a delivery to reach a customer after an order has been placed on the app. From an earlier beta test in Singapore, there were 10,000 deliveries made. Do you have enough training data to create an accurate ETA model?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284edd5-33a9-40d0-9a1f-81155649ec49",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "\"Accurate\" is subjective. Therefore follow the below steps:\n",
    "\n",
    "1. ***Clarify what is \"good\" enough***\n",
    "* What will the prediction be used for? -> get context -> accuracy for order-driver matching may need to be higher than for customers\n",
    "* What errors are acceptable? -> for customers, better to overestimate delivery than underestimate\n",
    "* Benchmark performance for ETA -> establish accuracy provided in other markets\n",
    "  \n",
    "2. ***Assess baseline performance***\n",
    "* Develop a baseline model using the 10,000 deliveries\n",
    "* This is a regression problem therefore try:\n",
    "  * Multi-linear regression: preperation time + distance\n",
    "  * Assess performance using RMSE, MAE, R2\n",
    "\n",
    "3. ***Determine how additional data improves accuracy***\n",
    "* Choose an evaluation metric (e.g. R2) and build learning curves to assess how performance changes with increasing % of data\n",
    "* If the learning curve begins to plateau then more data might not be required - focus on optimisation (i.e. feature selection, regularization etc.)\n",
    "\n",
    "If more data is required as performance isn't good enough. Follow the below steps:\n",
    "\n",
    "1. ***Assess features***\n",
    "* Can we add additional features (e.g. traffic patterns, supply & demand)\n",
    "* Are there are almost as many or more featurs than data points, if so the model will be prone to overfitting - apply PCA or feature selection\n",
    "\n",
    "2. ***Alternative model***\n",
    "* Do alternative models cope better with smaller training datasets?\n",
    "\n",
    "3. ***Assess impact***\n",
    "* Is the less accurate prediction a true launch blocker?\n",
    "* If not, launch in the new market and retrain the model using the generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3c4b9-fd94-4a85-99fb-bd6b34713ac9",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd24659-01e8-4208-9896-cc9cf7e9eeec",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f3ebb-2b35-4382-9ea3-516a0fe923b3",
   "metadata": {},
   "source": [
    "#### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b63d7-7a83-412e-87d8-4cdf8c1c1056",
   "metadata": {},
   "source": [
    "**12. Describe precision and recall and give their formulas. What is their importance and what is the nature of the trade-off between the two?**\n",
    "\n",
    "Source: Data Lemur, Machine Learning Question \"Precision/Recall trade-off\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda555b-4b34-49a9-a15b-2709a3f1d639",
   "metadata": {},
   "source": [
    "**_What are they?_**\n",
    "\n",
    "* Precision measures the proportion of predicted positive classes that are actually positive (i.e. confidence in predicted value):\n",
    "\n",
    "    $\\text{Precision: } \\frac{TP}{TP + FP}$\n",
    "\n",
    "* Recall measures the proportion of the positive class instances that are correctly classified (i.e. true positive rate)\n",
    "\n",
    "    $\\text{Recall: } \\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8856aff-c251-4893-833b-3e77a0b7cc59",
   "metadata": {},
   "source": [
    "**_What is their importance?_**\n",
    "\n",
    "* Measuring model performance with imbalanced data using accuracy isn't appropriate.\n",
    "\n",
    "    $\\text{Accuracy: } \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "* Precision and recall are widely used metrics to evaluate the performance of an imbalanced classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1108b30-54e0-4655-9821-bf814ce48e0a",
   "metadata": {},
   "source": [
    "**_Trade-off:_**\n",
    "\n",
    "* Precision and recall are not particularly useful metrics when used in isolation.\n",
    "\n",
    "* It is possible to have perfect recall by simply predicting positive for everything. Likewise, precision can be maximised by only predicting positives for a very small number of extremely likely items.\n",
    "\n",
    "* Therefore they are often combined via the F1 score (harmonic mean):\n",
    "\n",
    "    $\\text{F1: } \\frac{2 \\times P \\times R}{P + R}$\n",
    "\n",
    "* The trade-off needs to be considered when optimising models (including choosing classification thresholds) for different problems.\n",
    "\n",
    "*Examples:*\n",
    "\n",
    "1. Fraudulent transaction identification\n",
    "\n",
    "* The model outputs the probability of a transaction being fraudulent\n",
    "* The cost of a fraudulent transaction is much higher than the cost involved in blocked but regular transactions\n",
    "* The classification threshold should therefore be set so False Negatives are minimised and recall maximised!  \n",
    "\n",
    "2. Youtube recommendatioms \n",
    "* The model outputs the probability of a film being relevant\n",
    "* False negatives are less of a concern\n",
    "* The classification threshold should therefore be set so True Positives and precision are maximised!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606292e-6829-4792-9759-fda14b48a980",
   "metadata": {},
   "source": [
    "**_Precision-recall curve:_**\n",
    "\n",
    "* As we lower a classification threshold (moving to the right on the plot), the recall will increase (as the number of FNs decreases) but the precision decreases (as the number of FPs increases)\n",
    "\n",
    "* As we increase a classification threshold (moving to the left on the plot), the precision will increase (as the number of FPs decreases) but the recall decreases (as the number of FNs increases)\n",
    "\n",
    "<img src=\"figures/precision_recall_curve.png\" align=\"center\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef487f9-4634-468b-a422-5c41abcce034",
   "metadata": {},
   "source": [
    "**_Random classifier:_**\n",
    "\n",
    "* A random classifier, for a binary task, outputs the positive class with probability equal to $p$\n",
    "* The above precision-recall curve shows precision is constant w.r.t the classification threshold but recall decreases as the classification threshold decreases. Why?\n",
    "\n",
    "* For a random classifier, precision is equal to the proportion of positive instances in our data (i.e. fixed):\n",
    "\n",
    "    $\\text{Precision: } \\frac{TP}{TP + FP} = \\frac{p \\cdot P}{p \\cdot P + p \\cdot N} = \\frac{P}{P+N}$\n",
    " \n",
    "* Whilst recall is equal to $p$:\n",
    "\n",
    "    $\\text{Recall: } \\frac{TP}{TP + FN} = \\frac{p \\cdot P}{p \\cdot P + (1-p) \\cdot P} = \\frac{p \\cdot P}{p \\cdot P + P -p \\cdot P} = \\frac{p \\cdot P}{P} = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fe0fe-b438-4133-b976-cd1244216b8e",
   "metadata": {},
   "source": [
    "[Above explanation](https://stats.stackexchange.com/questions/89495/precision-and-recall-of-a-random-classifier)\n",
    "\n",
    "[Superior explanation](https://stats.stackexchange.com/questions/251175/what-is-baseline-in-precision-recall-curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e301a5-e91c-4ae0-a9c6-58832312e2bc",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8195f15-b683-4a71-b1ae-720ea92641ed",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608249b-bf04-40d9-baab-643c3a70518a",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875efa5c-4222-4ae8-9103-115520a0f057",
   "metadata": {},
   "source": [
    "**13. What are the assumptions underlying linear regression?**\n",
    "\n",
    "Source: Data Lemur, Machine Learning Question \"Assumptions of linear regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3e047-b62d-435e-9c04-38837f02f752",
   "metadata": {},
   "source": [
    "**_Model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea257a5-f672-4075-97a1-f714f85cd624",
   "metadata": {},
   "source": [
    "* We assume the below linear model exists:\n",
    "\n",
    "    $Y = b_0 + b_1X_1 + b_2X_2 + ... + b_pX_p + e$\n",
    "\n",
    "* Where fitted values are given by:\n",
    "\n",
    "    $\\hat{Y} = \\hat{b}_0 + \\hat{b}_1X_1 + \\hat{b}_2X_2 + ... + \\hat{b}_pX_p$\n",
    "\n",
    "Note: A constant (b<sub>0</sub>) will be added to all models with a view to reducing bias (as this will force the mean of the residuals to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565ebe-2c38-452f-b1f7-142fe62c2496",
   "metadata": {},
   "source": [
    "**_Matrix representation_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852aae89-06f1-407b-810e-3e315a02e268",
   "metadata": {},
   "source": [
    "* The goal is to minimise the residual for the following function:\n",
    "\n",
    "    $\\hat{Y} = X\\beta$\n",
    "\n",
    "    Where $X$ is a matrix of predictor variables and $\\beta$ is a vector of parameters that determines the weight of each variable in predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b97fa6-ece6-48f2-864d-e79f4a60948f",
   "metadata": {},
   "source": [
    "\\(\\begin{bmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{bmatrix} = \\begin{bmatrix} X_{11} & X_{12} & \\cdots & X_{1p} \\\\ X_{21} & X_{22} & \\cdots & X_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ X_{n1} & X_{n2} & \\cdots & X_{np} \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988caabd-59e9-4823-90b0-f104ea4e9db2",
   "metadata": {},
   "source": [
    "**_A) Linearity_**\n",
    "\n",
    "1) *The regression model is linear in the coefficients and the error term (i.e. all raised to the power of one)*\n",
    "\n",
    "    Note: Linear regression models can model curvature by including nonlinear *variables* such as polynomials (e.g. X<sub>2</sub>). They don't need to be linear in the variables:\n",
    "   \n",
    "    $\\hat{Y} = \\hat{b}_0 + \\hat{b}_1X_1 + \\hat{b}_2X_2^2$\n",
    "\n",
    "   [Source](https://datascience.stackexchange.com/questions/12274/what-does-linear-in-parameters-mean)\n",
    "\n",
    "3) *No independent variable is a perfect linear function of other explanatory variables*\n",
    "\n",
    "   Note: Ordinary least squares cannot distinguish one variable from the other when they are perfectly correlated. This prevents the model fitting and will result in an error message. Imperfect but strong correlation (i.e. multicollinearity) leads to unstable coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203462aa-4279-4f5c-87b0-357189a15baa",
   "metadata": {},
   "source": [
    "**_B) Homoscedasticity_**\n",
    "1) *The error term has constant variance (no heteroscedasticity)*\n",
    "\n",
    "   Note: The variance of the error observations should be constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109b26d-f059-4437-a805-db3f715c1aac",
   "metadata": {},
   "source": [
    "**_C) Independence_**\n",
    "1. *All independent variables are uncorrelated with the error term (AKA Exogeneity)*\n",
    "\n",
    "    Note: If this is violated, it means the error term isn't unpredictable random error as we can predict it using the independent variables. It can indicate a missing variable or measurement error. It leads to bias in the coefficient estimates.\n",
    "\n",
    "2. *Observations of the error term are uncorrelated with each other*\n",
    "\n",
    "   Note: Observations of the error term should not have a natural sequential order (i.e. no autocorrelation). One observation of error term should not be predictive of the next observation. If this occurs, additional information can be incorporated into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958d0b9-9871-4f42-ad44-608607e650a0",
   "metadata": {},
   "source": [
    "**D) Normality**\n",
    "\n",
    "1) *The error term is normally distributed (optional)*\n",
    "\n",
    "   Note: This is crucial for accurate estimates of confidence/prediction intervals and p-values for coefficient estimates. However, its not required for unbiased estimates.\n",
    "\n",
    "2. *The error term has a mean of zero (optional)*\n",
    "\n",
    "   Note: When the constant term (b<sub>0</sub>) is included, the mean of the residuals will be forced to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db095f4d-19af-4c9b-b968-85fd1c14f052",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85de8f-4ffe-42c3-bb73-abd66ae8bf42",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb520a-fdd2-481e-a4a1-8c6ebddf60dc",
   "metadata": {},
   "source": [
    "## Medium questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2fe48-7424-4c1a-b319-ef9cb5e1e2b6",
   "metadata": {},
   "source": [
    "#### Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2c512-c6f4-4e09-96c1-6b954058c0bb",
   "metadata": {},
   "source": [
    "**14. Say we we are running a binary classification loan model, and rejected applicants must be supplied with a reason why they were rejected. Without digging into the weights of features, how would you supply these reasons?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe695c-22e5-4856-ab07-901b4d8fc036",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "* Use partial dependence plots (AKA PDPs or response curves)\n",
    "* These show the marginal effect of one or two features on the average output of a model (without explicity looking at feature weights)\n",
    "* This can highlight various relationships between the target and a feature (e.g. linear, monotonic or more complex)\n",
    "* You can then supply reasons accordingly\n",
    "\n",
    "*How it works:*\n",
    "* Train the model normally\n",
    "* Estimate the partial dependence function for each feature\n",
    "* The partial dependence function at a particular feature value represents the average (marginal) prediction if we force all data points to assume that feature value\n",
    "* Steps involved:\n",
    "  \n",
    "      i. Select a feature (or pair of features)\n",
    "      ii. Set the value for the feature(s). Calculate the average model output across all training inputs. Where all other features are fixed at their true value for the particular training instance.\n",
    "      iii. Repeat steps 1-2 for a range of feature values, thereby building a PDP.\n",
    "\n",
    "$$ \\hat{f}_s(x_s) = \\frac{1}{N} \\sum_{n=i}^n \\hat{f}(x_s, x_C^{i}) $$\n",
    "<center> Where, x<sub>s</sub> represents the given values for features of interest (in set S), x<sub>C</sub><sup>(i)</sup> are actual feature values from the dataset for the features in which we are not interested, and n is the number of instances in the dataset. </center>\n",
    "\n",
    "*Advantages:*\n",
    "* Easy to implement\n",
    "* Intuitive. In the uncorrelated case, the interpretation is clear. It shows how the average prediction in your dataset changes when the j-th feature is changed\n",
    "\n",
    "*Disadvantages:*\n",
    "* The realistic maximum number of features in a partial dependence function is two\n",
    "* The assumption of independence. It is assumed that the feature(s) for which the partial dependence is computed are not correlated with other features. When the features are correlated, we create new data points in areas of the feature distribution where the actual probability is very low.\n",
    "* Heterogeneous effects might be hidden because PD plots only show the average marginal effects. Therefore plot standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6909b3-4904-4974-9d2f-d7a5e5b807c6",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a568640-90bc-4818-a0b7-18de8f4fb3df",
   "metadata": {},
   "source": [
    "#### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca0dd5-404f-4eaa-84f2-c4420c04ecc1",
   "metadata": {},
   "source": [
    "**15. Say you are given a very large corpus of words. How would you identify synonyms?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad981df6-c037-4e5d-9b7e-4b8fbf3a5ede",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "Two steps:\n",
    "\n",
    "1. Generate contextual word embeddings\n",
    "* Use an algorithm such as Word2Vec to embed the words\n",
    "* This will produce context dependent vector representations for each word\n",
    "* The distance between the vectors can be used to estimate similarity. For example, via cosine similarity or some other similar measure.\n",
    "  \n",
    "2. Apply non-supervised learning\n",
    "* K-means clustering can be used to identify clusters of word embeddings\n",
    "* K-nearest neighbours can be used to find synonyms for a specific word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748622a6-8287-4b71-b5de-800363cf05ac",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d65b4f-607a-4d1b-a583-ed1ddb72d1ab",
   "metadata": {},
   "source": [
    "#### Bias-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13061e66-23aa-4221-add4-f32fb8c25916",
   "metadata": {},
   "source": [
    "**16. What is the bias-variance trade-off? How is it expressed using an equation?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414e51d-1731-4d89-afcd-450f6cd83133",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "$$ E(x) = Bias + Variance + Error_{irreducible} $$\n",
    "<center> Where E(x) is the generalisation error for a point x. </center>\n",
    "\n",
    "\n",
    "The trade-off exists as we want low bias and variance. This means preventing overfitting whilst capturing the relationship between the input features and target.\n",
    "\n",
    "*Bias*\n",
    "* The error that occurs when a model underfits data\n",
    "* Flexible models tend to have low bias\n",
    "* Rigid models tend to have high bias\n",
    "* The high bias the model is too simplistic and may not capture the relationship between input features and the target.\n",
    "* Example high bias model: Linear regression model where the underlying relationship is not linear.\n",
    "\n",
    "*Variance*\n",
    "* The error that occurs when a model overfits data\n",
    "* Rigid models tend to have low variance\n",
    "* Flexible models tend to have high variance\n",
    "* High variance means a model is susceptible to changes in training data. This indicates it is capturing the noise.\n",
    "* Example high variance model: A complex neural network when the underlying relationship between features and the target is linear.\n",
    "\n",
    "*Irreducible error*\n",
    "* Error that cannot be addressed directly by the model (e.g. from measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588aa37-4c17-45f9-9bbb-657918353a36",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdf2ef-9118-4442-953f-b4d3b311c07f",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca216b2b-bb5e-491d-85de-6b04b73e8e9c",
   "metadata": {},
   "source": [
    "**17. Define the cross-validation process. What is the motivation behind using it?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c82d42-aec3-4871-949f-35119c4ff087",
   "metadata": {},
   "source": [
    "**_Answer:_**\n",
    "\n",
    "* Cross-validation is a technique used to asses the performance of an algorithm\n",
    "* Motivation:\n",
    "1. Avoiding training & testing on the same subset of data, which would lead to overfitting\n",
    "2. Avoiding using a dedicated validation set, with which no training can be done - very useful for small datasets\n",
    "* The process is as follows:\n",
    "\n",
    "      i. Randomly shuffle data into *k* equally-sized blocks (folds)\n",
    "      ii. For each i in fold 1,2...k, train the model on all the data except for fold *i*. Evaluate the validation error using the ith fold.\n",
    "      iii. Average the *k* validation errors from step 2 to get an estimate of the generalisation error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569f378-63bd-4edf-9878-ecf650202234",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0d295-0908-4779-bcca-79d592e8a42b",
   "metadata": {},
   "source": [
    "#### Lead scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc9cf1-6ef6-442a-af8a-22b0eeb1a2eb",
   "metadata": {},
   "source": [
    "**18. How would you build a lead scoring algorithm to predict whether a prospective company is likely to convert into being an enterprise customer?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6adfc-0db9-4b57-8cd0-340ade093774",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa942ff0-85e4-4dac-abf9-bd10e9790e55",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ebd10-61ac-4947-afc5-5ba6e1c3baf7",
   "metadata": {},
   "source": [
    "#### Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe5039-e6df-4bc2-b119-4445575445dd",
   "metadata": {},
   "source": [
    "**19. How would you approach creating a music recommendation algorithm?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee3681-bdc0-432c-a833-c2264fc64b51",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f644cbc-3e1c-48ca-81ed-4bdfa2f39a40",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bef0b-5890-4b71-a702-505086d18f3f",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a072c7-9fa3-4237-a02c-0cb40baf2708",
   "metadata": {},
   "source": [
    "#### Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d579442a-1a8f-41a2-9fda-7550dfadbbcf",
   "metadata": {},
   "source": [
    "**20. Define what it means for a function to be convex. What is an example of a machine learning algorithm that is not convex and describe why that is so?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14255c13-99fd-4daa-afa1-c2f42b656dc8",
   "metadata": {},
   "source": [
    "**_Convex functions:_**\n",
    "\n",
    "Informal definition:\n",
    "\n",
    "* Convex functions have a global minima.\n",
    "\n",
    "* A strictly convex function on an open set has no more than one minimum.\n",
    "\n",
    "Formal definition:\n",
    "\n",
    "* For any two points $x_1$ and $x_2$ in the domain of $f$: \n",
    "\n",
    "    $f(t \\cdot x_1+(1-t) \\cdot x_2)) \\leq t \\cdot f(x_1)+(1-t) \\cdot f(x_2)$, where $ 0 \\leq t \\leq 1$\n",
    "\n",
    "* The line connecting $x_1$ and $x_2$ must always be greater than or equal to the function $f$ between $x_1$ and $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa35272-6c80-45d1-ab1b-0d7b56f4a741",
   "metadata": {},
   "source": [
    "<img src=\"figures/convex_function.png\" align=\"center\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367fa6a-3bc2-4632-a4c9-fe00ca27a090",
   "metadata": {},
   "source": [
    "Importance:\n",
    "\n",
    "* Convexity matters as it determines the number of minima.\n",
    "\n",
    "* This has implications when optimising a function $h(x)$ during machine learning (when we search for the best model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfd99b-a2b1-4331-8365-6a54b1b54b09",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "1. Neural networks\n",
    "\n",
    "* Neural networks are universal function approximators (with a sufficient number of neurons)\n",
    "\n",
    "* Not all functions are convex, therefore not all neural networks will approximate convex functions\n",
    "\n",
    "* How do we know if neural network is approximating a non-convex function $h(x)$?\n",
    "\n",
    "* If you can switch parameter weights between nodes and get the same cost function output with the same model inputs, there must be local minima.\n",
    "\n",
    "2. Quadratic functions are convex\n",
    "\n",
    "    $f(x) = x^2$\n",
    "\n",
    "4. Exponential functions are convex\n",
    "\n",
    "    $f(x) = e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76905c-b848-49b3-8a4c-ac30cbe576a8",
   "metadata": {},
   "source": [
    "**_Concave functions:_**\n",
    "\n",
    "Informal definition:\n",
    "\n",
    "* Concave functions have a global maxima.\n",
    "\n",
    "* A strictly concave function on an open set has no more than one maxima. \n",
    "\n",
    "Formal definition:\n",
    "\n",
    "* If $-f(x)$ is convex then $f(x)$ is concave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3dad1-5e8b-4176-806a-1e69c66bff25",
   "metadata": {},
   "source": [
    "**_Distinguishing concave and convex functions_**\n",
    "\n",
    "* The second derivative is the rate of change of the first derivative:\n",
    "\n",
    "    $f''(x)= \\frac{d}{dx} (\\frac{d}{dx}f(x))$\n",
    "\n",
    "* It can be used to classify extrema:\n",
    "\n",
    "    Let $f(x)$ be a function with $f'(x_0)=0$. If $f''(x_0)>0$ then the function has a local minimum at $x_0$. If $f''(x_0)<0$ then the function has a local maximum at $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f73da-562a-414a-b8fa-85986e63dd06",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6785f39-7bda-419d-a1dc-6b550635d4d3",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c09102-101a-400b-9a38-a72a422f1869",
   "metadata": {},
   "source": [
    "#### Entropy & information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b4f4f-200b-4b60-a4e5-dd9754f2efb7",
   "metadata": {},
   "source": [
    "**21. Explain what information gain and entropy are in the context of a decision tree and walk through a numerical example.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d5ea1-ded1-4238-b801-3ce77acb9354",
   "metadata": {},
   "source": [
    "**Information theory background**\n",
    "\n",
    "Entropy:\n",
    "\n",
    "* The entropy of a random variable $X$ is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to a variable's possible outcomes\n",
    "\n",
    "    $H(X) = \\mathbb{E}[-log_2p(X)]$\n",
    "\n",
    "* The entropy for a discrete random variable $X$ with $n$ possible values: \n",
    "\n",
    "    $H(X) = \\sum_i^n -p(x_i)log_2p(x_i)$\n",
    "\n",
    "* The entropy of a homogenous partition is zero!\n",
    "\n",
    "Information gain:\n",
    "\n",
    "* The amount of information gained about a random variable $X$ from observing another random variable $A$ taking a value $A=\\alpha$\n",
    "\n",
    "    $IG = \\text{Entropy}_{before} - \\text{Entropy}_{after}$\n",
    "\n",
    "    $IG(X, \\alpha) = H(X) - H(X | A=\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fed4d8-f7ec-4126-b44c-e5a801be3467",
   "metadata": {},
   "source": [
    "**_Decision tree_**\n",
    "\n",
    "* Assume we training a binary classifier to distinguish $blue$ and $green$ samples in the dataset below.\n",
    "\n",
    "* We want to minimise the entropy at a leaf. This will result in more homogenous subpartition and superior performance.\n",
    "\n",
    "* Therefore we want to maximise information gain when choosing splits.\n",
    "\n",
    "<img src=\"figures/entropy_example.png\" align=\"left\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6637c5d-5fce-4738-b440-39ac1306b3cb",
   "metadata": {},
   "source": [
    "* Assume a split on attribute $X=\\alpha$\n",
    "\n",
    "* This imperfect split breaks our dataset into a left branch (4 blues) and right branch (1 blue, 5 greens):\n",
    "\n",
    " <img src=\"figures/information_gain_example.png\" align=\"center\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145132ad-3c2a-4bfc-98e9-61525cadbe44",
   "metadata": {},
   "source": [
    "* Entropy before the split:\n",
    "\n",
    "    $H(Y) = \\sum_i^n -p(y_i)log_2p(y_i)$\n",
    "    \n",
    "    $H(Y) = -0.5log_2(0.5) - 0.5log_2(0.5) = -0.5(-1) - 0.5(-1) = 1$\n",
    "\n",
    "* Entropy after the split (note: a weighted average of both partitions):\n",
    "\n",
    "    Left branch: $H(Y | X < \\alpha) = -1log_2(1) - 0log_2(0) = 0$\n",
    "        \n",
    "    Right branch: $H(Y | X > \\alpha) = -\\frac{5}{6}log_2(\\frac{5}{6}) - \\frac{1}{6}log_2(\\frac{1}{6}) = 0.65$\n",
    "\n",
    "    $\\text{Entropy}_{after} = \\frac{4 \\times 0 + 6*0.65}{10} = 0.065$\n",
    "\n",
    "* Information gain:\n",
    "\n",
    "    $IG = \\text{Entropy}_{before} - \\text{Entropy}_{after}$\n",
    "\n",
    "    $IG = 1 - 0.065 = 0.935$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f776403-5cc0-41fa-8b58-e57bf8186f92",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0af6-738f-4dd1-bf62-a8bbddfbdb45",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c73fa-833d-4e3f-b7db-73db2652fecc",
   "metadata": {},
   "source": [
    "#### L1 & L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ff78-b5a3-4fcb-b2a8-8bd4aeae5d81",
   "metadata": {},
   "source": [
    "**22. What are L1 and L2 regularization? What are the differences between the two?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebdafb-dccc-44f3-888f-b045c34d0535",
   "metadata": {},
   "source": [
    "**What are they?**\n",
    "\n",
    "* Both are regularization methods that prevent overfitting.\n",
    "\n",
    "* Where, model complexity is measured by the magnitude of the weight vector.\n",
    "\n",
    "* Regularisation is therefore achieved by minimising the coefficients (parameter weights) of the model.\n",
    "\n",
    "    Note: Predictors should be standardised before fitting the model as the L1/2-term is a function of the size of the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5c0ac-56d7-4276-9352-496bc4ccceb2",
   "metadata": {},
   "source": [
    "**L1 regularisation**\n",
    "\n",
    "* Uses the L1-norm to penalise model complexity\n",
    "\n",
    "* The L1 term is a function of the number and size of the coefficients:\n",
    "\n",
    "    $ ||W||_1 = \\sum_{j=1}^d |w_j| $\n",
    "\n",
    "    *Where, W is the d-dimensional vector of model parameters (i.e. weights), which includes a dummy variable x<sub>0</sub>=1 for the y-intercept.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188ec96-b637-4db5-8c99-06c3a5b6d1ff",
   "metadata": {},
   "source": [
    "**_Properties:_**\n",
    "\n",
    "* In practice regularisation is achieved by keeping the magnitude of the weight vector below a threshold (e.g. 1)\n",
    "\n",
    "    $\\sum_{j=1}^d |w_j| \\leq 1 $\n",
    "\n",
    "    E.g. $|w_1| + |w_2| < 1$\n",
    "\n",
    "* The area in which the optimal weight vector resides will therefore take the shape of a diamond centred on the origin\n",
    "\n",
    "    <img src=\"figures/l1_l2_constraint.png\" align=\"center\" width=\"250\" /> <img src=\"figures/l1_surface.png\" align=\"center\" width=\"350\" />\n",
    "\n",
    "* If gradient descent is used to optimise weights, L1 is more likely to to reduce coefficients to zero than L2 regularisation.\n",
    "\n",
    "* Why? The gradient is constant even as weights approach zero in magnitude.\n",
    "\n",
    "* L1 therefore tends to produce sparse solutions, which is useful for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a708d40-b1d9-42ad-b9d4-6f4dc5d38109",
   "metadata": {},
   "source": [
    "**_Example: LASSO regression_**\n",
    "\n",
    "* The goal is to choose the weights such that the objective function is minimised.\n",
    "\n",
    "* Objective function = MSE cost function + L1-norm:\n",
    "\n",
    "    $ \\min_{w} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2 + \\lambda||W||_1 $ \n",
    "\n",
    "    $ \\min_{w} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\sum_{j=0}^d w_j x_j)^2 + \\lambda \\sum_{j=1}^d |w_j|$ \n",
    "\n",
    "    *Where, lambda is the tuning parameter (larger values result in stronger regularization), N is the number of training samples, W is the d-dimensional vector of model parameters (i.e. weights), which includes a dummy variable x<sub>0</sub>=1 for the y-intercept.*\n",
    "\n",
    "    Note: In practice, the cost function often uses 1/2N to simplify implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07542fa9-0e4e-4602-aff5-e99893742d07",
   "metadata": {},
   "source": [
    "**L2 regularisation**\n",
    "\n",
    "* Uses the *squared* L2-norm to penalise model complexity\n",
    "\n",
    "* The L2 term is a function of the number and size of the coefficients. It is equivalent to Euclidian distance:\n",
    "\n",
    "    $ ||W||_2 = \\sqrt{\\sum_{j=1}^d w_j^2} $\n",
    "\n",
    "    *Where, W is the d-dimensional vector of model parameters (i.e. weights), which includes a dummy variable x<sub>0</sub>=1 for the y-intercept.*\n",
    "\n",
    "* The *squared* L2-norm is given by:\n",
    "\n",
    "    $ ||W||_2^2 = \\sum_{j=1}^d w_j^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80680e2-21c8-4c61-bc63-4a1950021441",
   "metadata": {},
   "source": [
    "**_Properties:_**\n",
    "\n",
    "* The L2-norm squares the individual model parameters. This means large weights are much more influential than smaller weights. Compared to L1 regularisation, L2 therefore prioritises minimising larger weights\n",
    "\n",
    "* In practice regularisation is achieved by keeping the magnitude of the weight vector below a threshold (e.g. 1)\n",
    "\n",
    "    $\\sum_{j=1}^d w_j^2 \\leq 1 $\n",
    "\n",
    "    E.g. $|w_1|^2 + |w_2|^2 < 1$\n",
    "\n",
    "* The area in which the optimal weight vector resides will therefore take the shape of a circle centred on the origin\n",
    "\n",
    "    <img src=\"figures/l1_l2_constraint.png\" align=\"center\" width=\"250\" /> <img src=\"figures/l2_surface.png\" align=\"center\" width=\"350\" />\n",
    "\n",
    "* If gradient descent is used to optimise weights, L2 is less likely to to reduce coefficients to zero than L1 regularisation.\n",
    "\n",
    "* Why? The cost surface is smoother. Also, the gradient decreases as weights approach zero in magnitude. This slows the rate at which a weight decreases in value. \n",
    "\n",
    "* L2 therefore tends lead to weights with similar sizes. Although weights can get very very small, L2 is less likely to reduce coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999d004-2848-4d0c-a456-023134ca135c",
   "metadata": {},
   "source": [
    "**_Example: Ridge regression_**\n",
    "\n",
    "* The goal is to choose the weights such that the objective function is minimised.\n",
    "\n",
    "* Objective function: MSE cost function + squared-L2-norm:\n",
    "\n",
    "    $ \\min_{w} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2 + \\lambda||W||_2^2 $ \n",
    "    \n",
    "    $ \\min_{w} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\sum_{j=0}^d w_j x_j)^2 + \\lambda \\left(\\sqrt{\\sum_{j=1}^d |w_j|^2}\\right)^2$ \n",
    "    \n",
    "    $ \\min_{w} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\sum_{j=0}^d w_j x_j)^2 + \\lambda \\sum_{j=1}^d |w_j|^2$ \n",
    "\n",
    "    *Where, lambda is the tuning parameter (larger values result in stronger regularization), N is the number of training samples, W is the d-dimensional vector of model parameters (i.e. weights), which includes a dummy variable x<sub>0</sub>=1 for the y-intercept.*\n",
    "\n",
    "    Note: In practice, the cost function often uses 1/2N to simplify implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b2a85-7658-42a2-b1ac-0627f1407caf",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756409e-e9d5-426a-a736-64834d113eab",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429feaad-653e-4055-86f5-ceff18341a4a",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e2845-5418-40c5-bff9-5f2dc6e79d4b",
   "metadata": {},
   "source": [
    "**23. Describe gradient descent and the motivations behind stochastic gradient descent.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ff341-86d9-4119-bc71-7b46608dc27d",
   "metadata": {},
   "source": [
    "**_Optimisation:_**\n",
    "\n",
    "* Models are mathematical functions $h(x)$ with parameters $w$ and hyperparameters. Where, hyperparameters are set before learning and parameters are optimised during learning.\n",
    "\n",
    "* Optimisation can be framed as a search through parameter space for optimal parameters.\n",
    "\n",
    "* This means optimal parameters are estimated by  minimising an objective function that measures model performance.\n",
    "\n",
    "* Why not use analytical methods such as ordinary least squares? They may not be available and dont tend to scale well with dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcc6ae-92a9-4434-aa8c-713c423327ca",
   "metadata": {},
   "source": [
    "**_Gradient descent_**\n",
    "\n",
    "* Gradient descent is a first order optimisation algorithm commonly used in machine learning.\n",
    "\n",
    "* Search for a minima in the cost surface (in parameter space) by descending the gradient using incremental changes in parameters.\n",
    "\n",
    "* Key steps: \n",
    "\n",
    "1. Define a loss, cost and objective function:\n",
    "\n",
    "    $J(\\boldsymbol{w}) = \\sum_i^n L(h_\\boldsymbol{w}(x_i), y_i)$\n",
    "\n",
    "2. Initialise the parameter vector $\\boldsymbol{w}$\n",
    "\n",
    "3. Pass data $\\{(x_i, y_i)^n\\}$ through the model function $h_\\boldsymbol{w}(x_i)$ and compute the cost/objective function $J(\\boldsymbol{w})$\n",
    "\n",
    "4. Compute the gradient of the cost/objective function with respect to the parameter vector $\\triangledown J(\\boldsymbol{w})$\n",
    "\n",
    "    $ \\triangledown J(\\boldsymbol{w}) = (\\frac{\\delta J}{\\delta w_1}, \\frac{\\delta J}{\\delta w_2}...)^T$ \n",
    "\n",
    "5. Update parameters using the following rule, where the learning rate $\\alpha_t$ determines the size of the step:\n",
    "\n",
    "    $\\boldsymbol{w^{t+1}} = \\boldsymbol{w^t} - \\alpha_t \\triangledown J(\\boldsymbol{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fd5a9-141b-42f2-bf94-82957f679ff5",
   "metadata": {},
   "source": [
    "Key points:\n",
    "\n",
    "* The goal is to move to the minimum of $h_\\boldsymbol{w}(x_i)$ where $\\frac{\\delta w}{\\delta J} = 0 $\n",
    "\n",
    "* The direction of the change is therefore determined by the sign of the gradient $\\triangledown J(\\boldsymbol{w})$\n",
    "\n",
    "<img src=\"figures/gradient_descent_direction.png\" align=\"center\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89ebc5-b3bf-450e-9ac3-e4fec091a3bc",
   "metadata": {},
   "source": [
    "**_Batch gradient descent:_**\n",
    "\n",
    "<img src=\"figures/batch_gradient_descent.png\" align=\"center\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5182cf0-09bc-46da-90eb-5ded5702df20",
   "metadata": {},
   "source": [
    "* Parameter updates occur after each epoch (i.e. all training samples have passed through the model).\n",
    "  \n",
    "* The time complexity of the algorithm is O(N), where N is the input of the sample.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Computation of gradients and parameter updates have O(N) time complexity. They can be slow with a lot of data.\n",
    "2. Convergence at local minima or saddle points can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0fd16-93be-4e9b-b2f5-23adde16e9b0",
   "metadata": {},
   "source": [
    "**_Mini-batch gradient descent:_**\n",
    "\n",
    "<img src=\"figures/mini_batch_gradient_descent.png\" align=\"center\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363bc9a-2d04-472a-8c99-dd981075ecca",
   "metadata": {},
   "source": [
    "* Parameters updated after a mini-batch of training samples (created after random shuffling). Batch size $b$ is a hyperparameter.\n",
    "\n",
    "* One epoch corresponds to $n/b$ iterations of the algorithm.\n",
    "\n",
    "Advantages:\n",
    "1. Easily fits in memory\r",
    "2. \n",
    "Smaller batches means noisier error gradients, which can improve generalisatio and avoid convergence at local minima.n3. \r\n",
    "Easy to parallelis\n",
    "\n",
    "Disadvantages:\n",
    "1. May not converge due to noise - instead oscillating around a minimum.\r\n",
    "e\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac55778-6082-4055-866c-71a5dc40ac40",
   "metadata": {},
   "source": [
    "**_Stochastic gradient descent:_**\n",
    "\n",
    "<img src=\"figures/stochastic_gradient_descent.png\" align=\"center\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129517c-88fe-4b1f-845a-b87bda5abd7b",
   "metadata": {},
   "source": [
    "* Parameters updated after every training sample (after random shuffling).\n",
    "\n",
    "* Assuming data is indepdent and identically distributed means, it provides an unbiased estimate of the gradient. \n",
    "\n",
    "* One epoch corresponds to $n$ iterations of the algorithm.\n",
    "\n",
    "Advantages:\n",
    "1. Easily fits in memory\n",
    "2. Very noisy error gradients, which can improve generalisation and avoid convergence at local minima.\n",
    "3. Easy to parallelise\n",
    "\n",
    "Disadvantages:\n",
    "1. Unstable gradients can mean it takes a long time to converge - instead oscillating round a minimum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca2927-fc0b-4b3b-bc92-c560e94c3cd6",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1319467-7860-4547-a32a-dfd3dbecaca0",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e7217-77b2-40c5-a8bd-895851ce77c0",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93763c9d-8816-43d9-a2dd-2928482efd70",
   "metadata": {},
   "source": [
    "**24. Assume we have a classifier that produces a score between 0 and 1 for the probability of a particular loan application being fraudulent. Say that for each application's score, we take the square root of that score. How would the ROC curve change? If it doesn't change, what kinds of functions would change the curve?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb054221-7344-468f-a501-a83c20d285de",
   "metadata": {},
   "source": [
    "**_ROC curve:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8970336-c958-43f0-9e04-9f2158b6fc87",
   "metadata": {},
   "source": [
    "<img src=\"figures/ROC_curves.png\" align=\"center\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b96202-7632-4162-bffb-43554c090f4e",
   "metadata": {},
   "source": [
    "Trade-off\n",
    "\n",
    "* As the classification threshold is lowered, $TPR$ increases because $TP$ increases and $FN$ decreases.\n",
    "\n",
    "    $TPR = \\frac{TP}{TP + FN}$\n",
    "\n",
    "* Unfortunately, as the classification threshold is lowered, $FPR$ also increases because $FP$ increases and $TN$ decreases.\n",
    "\n",
    "    $FPR = \\frac{FP}{FP + TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efcc89-63a0-45b7-92b6-b172c7b551b4",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "* If the classification threshold is also square rooted, the ROC curve will not change.\n",
    "\n",
    "* Why?\n",
    "\n",
    "    (1) The model still outputs scores such that $\\hat{h}(x) \\in [0,1]$.\n",
    "\n",
    "    (2) The square root function does not change the ordering of the scores. Consequently, the $TPR$ and $FPR$ will not change.\n",
    "\n",
    "* A non-monotonic functions would alter the ROC curve (e.g. $x^2$)\n",
    "\n",
    "    Note: A monotonic function is one which preserves order between sets (including reversing it). The first order derivative of a monotonic function does not change sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4ab28-957c-4562-9db5-90ae11830990",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016fc39-8656-49c0-b234-8c5c8170f2fb",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc3ce39-c7db-4406-bc0b-9e654e097d99",
   "metadata": {},
   "source": [
    "#### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ba216-1f67-4aff-8a56-d5d4eef3e284",
   "metadata": {},
   "source": [
    "**25. Say X is a univariate Gaussian random variable. What is the entropy of X?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108d722-f040-4974-94a0-037c8a502d18",
   "metadata": {},
   "source": [
    "**_Information theory background_**\n",
    "\n",
    "* Entropy can be extended to a continuous random variable $X$ with a continuous probability density function $f_X(x)$\n",
    "\n",
    "    $H(X) = \\mathbb{E}[-ln f_X(x)]$\n",
    "    \n",
    "    $H(X) = - \\int_{-\\infty}^\\infty f_X(x)ln f_X(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993218a7-909b-4a0c-ad48-1ee365edda8b",
   "metadata": {},
   "source": [
    "**_Gaussian probability density function_**\n",
    "\n",
    "$f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417351e-488d-43ff-b82b-6ef7e7cfdbf0",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "<img src=\"figures/entropy_gaussian_distribution.png\" align=\"center\" width=\"800\" />\n",
    "\n",
    "[Source](https://proofwiki.org/wiki/Differential_Entropy_of_Gaussian_Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121f0fc-755a-4f4a-8014-d4cc5c42e230",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa507f-97ba-42c6-bebd-6d456cf3eb11",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17215801-bb32-4e0a-b9ff-ba8d3bd298fb",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a34077-7894-45f8-9819-723073190b25",
   "metadata": {},
   "source": [
    "**26. How would you build a model to calculate a customer's propensity to buy a particular item? What are some pros and cons of your approach?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6ee4f-9a19-4f7c-92c1-572f0714f43c",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca0389-ad81-42d0-90f8-884e60410869",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e13026-2cd0-400a-b781-789011728bd0",
   "metadata": {},
   "source": [
    "#### Naive Bayes vs Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b386b-3f66-4b87-af1b-2f308bb48da7",
   "metadata": {},
   "source": [
    "**27. Compare and constrast Gaussian Naive Bayes (GNB) and logistic regression. When would you use one over the other?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06eeed9-87b6-4f97-b7a3-5580bbc8dd97",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "Advantages of Naive Bayes:\n",
    "\n",
    "* It requires only a small number of observations and does not require optimisation. This means it is faster to implement than logistic regression.\n",
    "\n",
    "Advantages of Logistic regression:\n",
    "\n",
    "* It does not make assumptions about $p(\\boldsymbol{X}| Y)$. This means it is more flexible than Naive Bayes, but such flexibility requires more data to avoid overfitting.\n",
    "\n",
    "When would you use one over the other:\n",
    "\n",
    "* Use Naive Bayes if there is little data and its modeling assumption are appropriate\n",
    "\n",
    "* Use logistic regression with larger datasets, as Naive Bayes suffers from the fact the assumptions made on $p(\\boldsymbol{X}| Y)$  are probably notcorrect. .\n",
    "\n",
    "    Note: If the assumptions hold exactly, i.e. the data is truly drawn from the distribution that we assumed in Naive Bayes, then Logistic Regression and Naive Bayes converge to the exact same result in the limit (but Naive Bayes will be faster as it doesn't require optimisation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232068d-18f9-47c3-b4c2-d154e55aef6e",
   "metadata": {},
   "source": [
    "**_Logistic regression:_**\n",
    "\n",
    "* Logistic regression is often referred to as the discriminative counterpart of Naive Bayes as it directly learns $p(Y|\\boldsymbol{X})$\n",
    "\n",
    "* As a linear model it won't work well when the decision boundary is not linear\n",
    "\n",
    "* It's relative simplicity makes it a high-bias and low-variance model\n",
    "\n",
    "* When features are highly correlated, the coefficients won't be as accurate (this can be addressed via regularisation, removal of features...)\n",
    "\n",
    "**Background**\n",
    "\n",
    "Sigmoid function:\n",
    "\n",
    "* The sigmoid function, also known as the logistic function, is defined as follows:\n",
    "\n",
    "    $\\forall z \\in R$, $g(z) \\in [0,1]$\n",
    "\n",
    "    $g(z)=\\frac{1}{1+exp-z}$\n",
    "\n",
    "* It outputs numbers between $0$ and $1$. At input $0$, it outputs $0.5$.\n",
    "\n",
    "    <img src=\"figures/sigmoid_func.png\" align=\"center\" width=\"300\" />\n",
    "\n",
    "Logistic regression:\n",
    "\n",
    "* In the linear regression model, we have modelled the relationship between outcome and features with a linear equation:\n",
    "    \n",
    "    $\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_pX_p$\n",
    "\n",
    "* The logistic regression model uses the sigmoid function to squeeze the output of a linear equation between $0$ and $1$:\n",
    "\n",
    "    $p(Y=1 | \\boldsymbol{X}; \\boldsymbol{\\beta)} = \\frac{1}{1+exp(-\\boldsymbol{\\beta}^T\\boldsymbol{X})}$\n",
    "\n",
    "Interpretation of weights:\n",
    "\n",
    "* The weights do not influence the probability linearly as the weighted sum $\\boldsymbol{\\beta}^T\\boldsymbol{X}$ is transformed by the logistic function to a probability\n",
    "\n",
    "* However, rearranging shows logistic regression model is a linear model for the log-odds (AKA logit):\n",
    "\n",
    "    $ ln(\\frac{p(Y=1)}{1-p(Y=1)}) = log(\\frac{p(Y=1)}{p(Y=0)}) = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_pX_p$\n",
    "\n",
    "* What does it all mean?\n",
    "\n",
    "\n",
    "Optimisation:\n",
    "\n",
    "* Gradient descent is used to optimise parameter weights\n",
    "\n",
    "* The loss function for logistic regression is called log-loss:\n",
    "\n",
    "    $L(\\boldsymbol{\\beta)}) = \\sum_{i=1}^n - Ylog(\\hat{Y}) - (1-Y)log(1-\\hat{Y})$\n",
    "\n",
    "    *Where $n$ is the number of records in the training set, $Y$ is the true label ($0\\text{ or }1$) and $\\hat{Y}$ is the predicted label ($0\\text{ to }1$)*\n",
    "\n",
    "* Given $Y$ is either $0\\text{ or }1$, the loss function reduces to $-log(\\hat{Y})$:\n",
    "\n",
    "    <img src=\"figures/log_loss.png\" align=\"center\" width=\"300\" />\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1. Linearity of independent variables and log-odds\n",
    "\n",
    "* Logistic regression assumes a linear relationship between the log-odds (AKA logit) of the dependent variable and the independent variables\n",
    "\n",
    "* The log-odds (AKA logit) is the logarithm of the odds ratio, where $p$ is probability of a positive outcome\n",
    "\n",
    "    $logit(p) = log(\\frac{p}{1-p})$\n",
    "\n",
    "  <img src=\"figures/logit.png\" align=\"center\" width=\"300\" />\n",
    "\n",
    "  *Plot of logit(x) in the domain of 0 to 1, where the base of the logarithm is e.*\n",
    "\n",
    "3. No strongly influential outliers\n",
    "\n",
    "4. Absence of Multicollinearity\n",
    "\n",
    "5. Independence of observations\n",
    "\n",
    "6. Sufficiently large sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3d17f-b216-4604-bb87-95bc671dc33c",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f72a6d-df40-4ae9-9d74-22de0989d7b4",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25209038-7391-4d84-b8ed-75a4e2a38359",
   "metadata": {},
   "source": [
    "## Hard questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f994f-7d42-4ad4-a837-fe193563f327",
   "metadata": {},
   "source": [
    "#### Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e622b1-9c55-44a2-b045-2d2a56587dd7",
   "metadata": {},
   "source": [
    "**28. Walk me through how you'd build a model to predict whether a particular user will churn.**\n",
    "\n",
    "Source: Data Lemur, Machine Learning Question \"Modeling user churn\" & Ace the Data Science Interview, Question 7.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4dcb5-01ac-490f-b3c8-4100d22d3741",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ac617-fd62-45fb-b776-bc818058eace",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605a17e-cfb6-4b89-9d88-1e7fee0a7f27",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53996cca-3298-43ca-91be-f63fec467d74",
   "metadata": {},
   "source": [
    "**29. What loss function is used in k-means clustering given k clusters and n sample points? Compute the update formula using (1) batch gradietn descent and (2) stochastic gradient descent for the cluster mean for cluster k using a learning rate c.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49eca1b-d8f8-4b0f-a301-8613d0b3e3cd",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d508a-cef8-4cc8-a016-396df1f9d78b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b583f3a-ee3c-4d69-99ec-8f00125a4c87",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceecb13-cf5a-4d05-b658-85597b6bf515",
   "metadata": {},
   "source": [
    "**30. Describe the kernel trick in SVMs and give a simple example. How do you decide which kernel to choose?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5c8c0-d4d7-4ad3-bc59-1b532483d872",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb6901-b27b-470f-a50f-7ad318d3fd6e",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc88f04-ed6a-4c76-bdb4-9b63f6e8afa4",
   "metadata": {},
   "source": [
    "**31. Say we have N observations for some variable which me model as being drawn from a Gaussian distribution. What are you best guesses for the parameters of the distribution?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d824c76-6a2f-43be-883f-e524616e75ab",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a78f8-7cd9-43a0-804e-1c19bf6a70a4",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb5b79-35b5-4b91-b08a-9411a19986ca",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53733ef-de33-44f5-8a50-dc7559af8535",
   "metadata": {},
   "source": [
    "**32. Say we are using a Gaussian mixture model (GMM) for anomaly detection of fradulent transactions to classify incoming transactions into K classes. Describe the model setup formulaically and how to evaluate the posterior probabilities and log likelihood. How can we determine if a new transaction should be deemed fraudlent?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa4d30-4013-4558-a253-b729069b6d64",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39051b-0cd5-49d4-84ef-9531219357b2",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f821aa-a4ff-4de0-9cbe-8cc4e9dd3334",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8eb6d-45b4-49bd-8a73-4c655be028e9",
   "metadata": {},
   "source": [
    "**33. Suppose you are running a linear regression and model the error terms as being normally distributed. Show that in this set-up maximizing the likelihood of the data is equivalent to minimizing the sum of the squared residuals.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccc923-6bf3-4594-a1f3-dc727922a421",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13908c-8933-4a0c-8647-8b7bfd873071",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfb669-79c6-46a9-9fae-5185bd6e947d",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33002f34-b66f-4c1a-b791-41c68c689de5",
   "metadata": {},
   "source": [
    "**34. Describe the idea behind principal component analysis and describe its formulation and derivation in matrix form. Next go through the procedural description and solve the constrained maximization.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2492-6189-4687-9d65-132ec1e2c470",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7582536-db11-40a3-9c28-e3051eb69e5b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b8254-c0f5-40e3-9127-7b92c4e2309f",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b2548-f946-4c37-888d-e1970e762f16",
   "metadata": {},
   "source": [
    "**35. Describe the model formulation behind logistic regression. How do you maximise the log-likelihood of a given model (using the two class case)?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737087e-f9ad-4402-b74b-2be8b962dea3",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2043186-09b7-449c-8bea-af58e6a5fd8f",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3fd0b-fb2e-43fa-9ac1-8c36232906c1",
   "metadata": {},
   "source": [
    "#### Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd3382-4967-4c7e-9065-40d36289863d",
   "metadata": {},
   "source": [
    "**36. How would you approach making a music recommendation algorithm for Discover weekly (a 30 song weekly playlist personalised to an individual user)?**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4b6f7-2762-490c-a6a8-ddc07e6e6926",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753214d-f643-4dfd-ba59-36e9df03b5b5",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d516d-3dd8-4dd5-aa0d-c7a3b7207339",
   "metadata": {},
   "source": [
    "#### Variance-covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f22f39-dcb8-43c4-8b7b-5bcc8f6cf962",
   "metadata": {},
   "source": [
    "**37. Derive the variance-covariance matrix of the least squares parameter estimates in matrix form.**\n",
    "\n",
    "Source: Ace the Data Science Interview, Question 7.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a7549-5917-481e-880e-fc675d313b26",
   "metadata": {},
   "source": [
    "**_Answer:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9335e8-496b-48dd-b849-38e1ed51e2a8",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6d611-47a4-4439-9344-a1f13aa06d4b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc38f29-bc92-4431-8896-acf8333b9fc8",
   "metadata": {},
   "source": [
    "## Multiple choice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426eed5-3a39-43d1-80c7-cecc663cb07f",
   "metadata": {},
   "source": [
    "**1. A $4$ input neuron has weights $3,4,2,1$. The activation function is linear with the constant of proportionality $2$. A bias of $10$ exists at the nodes.  The inputs are $4$, $10$, $20$ and $5$ respectively. What is the output?**\n",
    "1. 107\n",
    "\n",
    "2. 204\n",
    "  \n",
    "3. 184\n",
    "  \n",
    "4. 194"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015e597-b35c-49e6-96d0-e879de322aa3",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "$((4*3) + (10*4) + (20*2) + (5*1))*2 + 10 = 204$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41787f36-def5-4537-aa08-685ea119c185",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a9f90-f42c-4d17-9fb4-685510f3f228",
   "metadata": {},
   "source": [
    "**2. Which of the given modeling techniques is best suited for text classification if the number of points is less than $1000$ samples?**\n",
    "1. kernel approximation\n",
    "   \n",
    "2. naive bayes\n",
    "   \n",
    "3. logistic regression classification\n",
    "   \n",
    "4. k nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ad6fe-7f86-4c51-a98f-ff8ae419cc5f",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8665de8-8821-4ba9-8162-a4c2e9782e86",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa9e21-e625-46d2-abef-3167a421403c",
   "metadata": {},
   "source": [
    "**3. Why is the method of least squares the most accepted method for estimating linear parameters?**\n",
    "1. The least squares estimate of the coefficients of a linear model has the smallest variance among all linear-unbiased estimates.\n",
    "   \n",
    "2. The least square estimate of the coefficients of a linear model has the smallest variance among all linear-biased estimates.\n",
    "   \n",
    "3. The least square estimate of the coefficients of a linear model has the greatest variance among all linear-biased estimates.\n",
    "   \n",
    "4. The least square estimate of the coefficients of a linear model has the greatest variance among all linear-unbiased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217da05-f0cb-4467-a521-cbeecbe33315",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "* Answer 1:\n",
    "\n",
    "    \"The least squares estimate of the coefficients of a linear model has the smallest variance among all linear-unbiased estimates.\"\n",
    "\n",
    "* Why?\n",
    "\n",
    "    The method of least squares is widely accepted for estimating linear parameters because it provides estimates that minimize the sum of the squared differences between the observed and predicted values. This criterion leads to unbiased estimates of the parameters with the smallest possible variance among all linear-unbiased estimates. The least squares method is also computationally efficient and has nice mathematical properties, making it a preferred choice for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa77eb-0f04-40df-9fb1-a795e3cb58d4",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172426d2-59bd-4ef2-9f5d-ee911f06f6f8",
   "metadata": {},
   "source": [
    "**4. Which of the given options is true about the sum of two independent, normal random variables $X_1$ and $X_2$ with mean $_1$ and $_2$ and variance $_1^2$ and $_2^2$?**\n",
    "1. It is normal with mean $_1 + _2$ and variance $max(_1^2, _2^2)$\n",
    "   \n",
    "2. It is normal with mean $_1 + _2$ and variance $(_1+_2)^2$\n",
    "   \n",
    "3. It is normal with mean $_1 + _2$ and variance $_1^2 + _2^2$\n",
    "   \n",
    "4. It is not normal with mean $max(_1, _2)$ and variance $max(_1^2, _2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8128f-51c3-4398-abcd-e5d7c24552ab",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "* If $X_1, X_2,...,X_n$ are independent:\n",
    "\n",
    "    $Var(\\sum_{i=1}^nX_i) = \\sum_{i=1}^nVar(X_i)$\n",
    "\n",
    "* Therefore it is normal with mean $_1 + _2$ and variance $_1^2 + _2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316ce38-f9e2-430c-8cd4-417e8412a251",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adba4a5-4927-435c-b0b4-9d37daee3a8a",
   "metadata": {},
   "source": [
    "**5. Consider a set of discret numbers $S = {0, 1, 2, 3, 4, 5}$. You select a number $X_1$ randomly from $S$. Then you create a new set $S_1 = \\{X_1 \\in S \\text{ such that } X> X_1\\}$. Now you select a number $X_2$ randomly from $S_1$. What is the expected value of $X_2$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50959-700f-40ad-a43c-960018603e97",
   "metadata": {},
   "source": [
    "**_Background:_**\n",
    "\n",
    "* Let $X$ be a finite or countably infinite discrete random variable with range $R_X = \\{x_1, x_2, x_3,... \\}$. The expected value $E[X]$ is given by:\n",
    "\n",
    "    $E[X] = \\sum_{x_k\\in R_X}P(X=x_k) \\times x_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a100af-4e50-4fe1-bc8e-f6a589c03c56",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "* If $X_1 = 0$,  $S_2 = \\{ 1, 2, 3, 4, 5 \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 0] = \\frac{15}{5} = 3$ \n",
    "\n",
    "* If $X_1 = 1$,  $S_2 = \\{ 2, 3, 4, 5 \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 1] = \\frac{14}{4} = 3.5$ \n",
    " \n",
    "* If $X_1 = 2$,  $S_2 = \\{ 3, 4, 5 \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 2] = \\frac{12}{3} = 4$ \n",
    "\n",
    "* If $X_1 = 3$,  $S_2 = \\{ 4, 5 \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 3] = \\frac{9}{2} = 4.5$ \n",
    "\n",
    "* If $X_1 = 4$,  $S_2 = \\{ 5 \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 4] = \\frac{5}{1} = 5$ \n",
    "\n",
    "* If $X_1 = 5$,  $S_2 = \\{ \\}$:\n",
    "\n",
    "    $E[X_2 | X_1 = 5] = undefined$\n",
    "\n",
    "The overall expected value is the average of the above values:\n",
    "\n",
    "$E[X_2] = \\frac{3+3.5+4+4.5+5}{5} = 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9e5fe-eb9a-47a0-9647-3140a2e65a65",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c47dcb-a529-4b07-8020-9d81e2f0f380",
   "metadata": {},
   "source": [
    "**6. You decide to build a ridge regression model to solve a problem. You find that your data set has many features that are highly correlated and you decided to use principal component analysis (PCA) for dimensionality reduction. You also need to use cross-validation for the parameter (lambda) selection in ridge regression. What are the proper steps for this model-building tasks?**\r\n",
    "\r\n",
    "1. Divide the data into training and test sets. Partition the training data into training and validation sets and use cross-validation to select lambda. Use PCA on the training data to create a new of data that is of fewer dimensions. Make a ridge regression model using this low-dimension training data. Project the test data onto the same low-dimension space as that of the the transformed training data. Assess the model using the transforemd test data.\r\n",
    "\r\n",
    "2. Divide the data into training and test sets. Use PCA on the training data to create a new set of data that is of fewer dimensions. Apply cross-validation on the transformed training data by partitioning it into training and validation sets to select lambda. Apply PCA on the test data to transform them into data of the same dimension as the transformed training data. Assess the model using the transformed test data.\r\n",
    "\r\n",
    "3. Divide the data into training and test sets. Use PCA on the training data to create a new set of data that is of fewer dimensions. Apply cross-validation on the transformed training data by partitioning it into training and validatino sets to select lambda. Project the test data onto the same low-dimension space as that of the transformed training data. Assess the model using the transformed test data.\r\n",
    "\r\n",
    "4. Use PCA to create a new set of data that is of fewer dimensions. Divide the data into training and test sets. Apply cross-validation on the training data by partitioning it into training and validation sets to select lambda. Assess performance on the tes data. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d16615-4a18-43c9-9c15-12611576f724",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "* Answer 3:\n",
    "\n",
    "    \"Divide the data into training and test sets. Use PCA on the training data to create a new set of data that is of fewer dimensions. Apply cross-validation on the transformed training data by partitioning it into training and validatino sets to select lambda. Project the test data onto the same low-dimension space as that of the transformed training data. Assess the model using the transformed test data.\"\n",
    "\n",
    "* Why?\n",
    "\n",
    "    This approach ensures that the dimensionality reduction and lambda selection are performed on the training data, and the test data is appropriately transformed for evaluation, maintaining the consistency of the model-building process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5684c-f4ec-4d5c-b140-3b873b640db8",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e9d9e-c8d5-4fe2-879f-82cea16b3890",
   "metadata": {},
   "source": [
    "**7. Troy is building a classifier to determine whether children have been good or bad in the current year. While analysing his labelled data, Troy finds that the number of kids classified as bad are very low compared to those characterised as good. Troy does not believe that this is representative of the population and wishes to replicate the data points in class bad to ensure that his classifier trains correctly. What is the right way for Troy to go about this?**\r\n",
    "\r\n",
    "1. Divide the data into training and test sets and replicate the data of class bad only in the test set.\r\n",
    "\r\n",
    "2. Replicate the data of class bad and then divide them into training and test sets.\r\n",
    "\r\n",
    "3. Divide the data into training and test sets and replicate the data of class bad only in the training set.\r\n",
    "\r\n",
    "4. Divide the data into training and test sets and then replicate the data of class bad in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9c7e3-8915-4d6e-a284-ee21825c908f",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "* Answer 3:\n",
    "\n",
    "    \"Divide the data into training and test sets and replicate the data of class bad only in the training set.\"\n",
    "\n",
    "* Why?\n",
    "\n",
    "    This method ensures that the replication of data from the \"bad\" class is used only in the training set, not in the test set. The purpose of replicating data from the minority class is to balance the class distribution during the training phase without influencing the evaluation on unseen data. If the replicated data were also added to the test set, it could lead to optimistic performance estimates and potentially biased evaluations. Replicating data in the training set helps the classifier learn better from the minority class while maintaining the integrity of the test set for unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606988c-aa30-4482-a688-138adff893fd",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1328de-5103-4b21-ab49-f53682c50489",
   "metadata": {},
   "source": [
    "**9. James is an expert in modeling classification tasks using decision trees. While analysing the decision tree for a two-class problem, he observes a particular node X at which there are a total of 200 unclassified data points, with 50 data points belonging to the first class and the remainder belonging to the second class. Node X of the decision tree splits the data such that its left child has 80 data points and the right child has 120 data points. Among the 80 points on the left child, 20 belong to the first class and 60 to the second class. Among the 120 data points on the right child, 30 belong to the first class and the remainder belong to the second class. What conclusion will James draw about the split at node X?**\r\n",
    "\r\n",
    "1. The split is statistically significant.\r\n",
    "\r\n",
    "2. The split is not statistically significant.\r\n",
    "\r\n",
    "3. The significance of the split will be decided by the actual number of data points at the beginning of the learning process.\r\n",
    "\r\n",
    "4. The information given is not sufficient to decide the significance of the split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7c163-e759-4765-bb67-fa209c241083",
   "metadata": {},
   "source": [
    "**_Background:_**\n",
    "\n",
    "* To determine the significance of a split in a decision tree, statistical tests are often used.\n",
    "\n",
    "* A commonly used test is the chi-squared test for independence. This test assesses whether the observed distribution of classes in the child nodes is significantly different from what would be expected by chance (i.e random sampling)\n",
    "\n",
    "* Test statistic:\n",
    "\n",
    "    $ \\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i} $\n",
    "    \n",
    "    *Where $O_i$ is the observed value of interest and $E_i$ is the expected value.*\n",
    "\n",
    "* Hypotheses:\n",
    "\n",
    "    - Null hypothesis $H_0$:  No significant difference between the observed and expected distributions.\n",
    "\n",
    "    - Alternative hypothesis $H_1$:  Significant difference between the observed and expected distributions.\n",
    "\n",
    "* If the p-value of the test statistic is small enough ($<0.05$ by convention), then the null hypothesis is rejected.\n",
    "\n",
    "[Nice video](https://www.youtube.com/watch?v=8cwwewynQ6k&t=460s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3b3a0-5032-448e-9886-d4204a5a4b4a",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "<img src=\"figures/multiple_choice_q9.png\" align=\"center\" width=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a239f6-2500-4b44-beff-2764f22d2b76",
   "metadata": {},
   "source": [
    "* At node X:\n",
    "\n",
    "    $p(A)=\\frac{1}{4}$ and $p(B)=\\frac{3}{4}$\n",
    "\n",
    "* At left child:\n",
    "\n",
    "    $ \\chi^2 = \\frac{(20 - \\frac{1}{4}(80))^2}{\\frac{1}{4}(80)} + \\frac{(60 - \\frac{3}{4}(80))^2}{\\frac{3}{4}(80)} = \\frac{0}{20} + \\frac{0}{60} = 0$\n",
    "\n",
    "* At right child:\n",
    "\n",
    "    $ \\chi^2 = \\frac{320 - \\frac{1}{4}(120))^2}{\\frac{1}{4}(120)} + \\frac{(90 - \\frac{3}{4}(120))^2}{\\frac{3}{4}(120)} = \\frac{0}{30} + \\frac{0}{90} = 0$\n",
    "\n",
    "* Overall test statistic:\n",
    "\n",
    "    $ \\chi^2 = 0 + 0$\n",
    "\n",
    "* The observed distribution is equivalent to the expected distribution. Consequently the split is not statistically significant and the purity of the nodes is not improved.\n",
    "\n",
    "    Note: If they weren't equivalent, a p-value would be necessary to assess statistical significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f94f7-c0cb-4498-9765-7864fe7b3dd1",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548627b-fed9-43a2-b9d0-764fe146cdb7",
   "metadata": {},
   "source": [
    "**10. Jane collects data from a medical test she has devised to assess laziness. This test has two possible outcomes: positive and negative. She finds in her data that if a person is lazy, the test comes out positive 85% of the time and negative 15% of the time. If the person is in fact not lazy, the test comes out positive 2% of the time and negative 98% of the time time. She discovers from the literature on the subject that 30% of the overall population is lazy. A high school student has undergone Jane's medical test and has tested positive. What is the probability that the high School student is lazy?**\r\n",
    "\r\n",
    "1. 0.133\r\n",
    "\r\n",
    "2. 0.978\r\n",
    "\r\n",
    "3. 0.255\r\n",
    "\r\n",
    "4. 0.948"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccfe90-b925-440a-8057-a413392ff763",
   "metadata": {},
   "source": [
    "**_Bayes theorem:_**\n",
    "\n",
    "$P(B_1|A) = \\frac{P(A|B_1)P(B_1)}{P(A)} = \\frac{P(A|B_1)P(B_1)}{\\sum_i P(A|B_i)P(B_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2442b61-cadb-47b1-beac-edc03407b5b8",
   "metadata": {},
   "source": [
    "**_Set-up:_**\n",
    "\n",
    "* $B_1$: Is lazy\n",
    "\n",
    "    $P(B_1) = 0.3$\n",
    "\n",
    "* $B_2$: Is not lazy\n",
    "\n",
    "    $P(B_2) = 0.7$\n",
    "\n",
    "* $A$: Positive test\n",
    "\n",
    "    $P(A|B_1) = 0.85$\n",
    "\n",
    "    $P(A|B_2) = 0.02$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22653d22-1a6c-4b8b-b42e-671750777db8",
   "metadata": {},
   "source": [
    "**_Solution:_**\n",
    "\n",
    "$P(B_1|A) = \\frac{P(A|B_1)P(B_1)}{P(A|B_1)P(B_1) + P(A|B_2)P(B_2)}$\n",
    "\n",
    "$P(B_1|A) = \\frac{0.85 \\times 0.3}{0.85 \\times 0.3 + 0.0.02 \\times 0.7} = 0.948$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71845e4b-bcdd-4eba-a56a-89723727c026",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b859-67cb-4c32-bb65-b0f8da84da37",
   "metadata": {},
   "source": [
    "**11. X is a discrete random variable that can take values from the set ${-2, -1, 0, 1, 2}$. The below values plot the probability mass function of $X$. What is the expected value of the random variable $X^2$?**\n",
    "\n",
    "* P(X=-2) = 0.2\n",
    "* P(X=-1) = 0.25\n",
    "* P(X=1) = 0.25\n",
    "* P(X=2) = 0.3\n",
    "\n",
    "1. 1.45\n",
    "\n",
    "2. 0.04\n",
    "\n",
    "3. 2.5\n",
    "\n",
    "4. 0.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a361c8-2d41-4490-89e0-9daf34416773",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "* Let $X$ be a finite or countably infinite discrete random variable with range $R_X = \\{x_1, x_2, x_3,... \\}$. The expected value $E[X]$ is given by:\n",
    "\n",
    "    $E[X] = \\sum_{x_k\\in R_X}P(X=x_k) \\times x_k$\n",
    "\n",
    "* Substitute in the above values:\n",
    "\n",
    "    $ E[X] = (-2 \\times 0.2^2) + (-1 \\times 0.25^2) + (1 \\times 0.25^2) + (2 \\times 0.3^2) = 2.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df77adb-468d-49b9-8f05-282a0cf4f628",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e42941-f80a-4f63-88ce-c68de21bbac4",
   "metadata": {},
   "source": [
    "**12. Look at the Lagrangian formulation of the SVM optimisation equation. Which of the following statements are true about the solution of Lagrange multipliers (<sub>n</sub>)?**\n",
    "\n",
    "$\\text{Minimise: } L(w, b, ) = \\frac{1}{2}w^tw - \\sum_{n=1}^{n=N} _n(y_n(w^tx_n +b)-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffc054-fdef-42c5-9972-27b7fc31e4bb",
   "metadata": {},
   "source": [
    "1. The value of <sub>n</sub> is either 0 or 1.\n",
    "\n",
    "2. The value of <sub>n</sub> is either 0 or a positive number.\n",
    "\n",
    "3. A nonzero value for <sub>n</sub> implies that X<sub>n</sub> is a support vector.\n",
    "\n",
    "4. A zero value for <sub>n</sub> implies that X<sub>n</sub> touches the margin boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d10a58-d5a6-42ae-b3fd-d61c3997842d",
   "metadata": {},
   "source": [
    "1. (2) and (3)\n",
    "   \n",
    "2. (2) and (4)\n",
    "\n",
    "3. (2), (3) and (4)\n",
    "\n",
    "4. (1), (2), (3) and (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e0c79-eb46-43b6-8aa1-bca1451081fc",
   "metadata": {},
   "source": [
    "**_Solution_**\n",
    "\n",
    "* GPT says (2) and (4) are true:\n",
    "\n",
    "    - The value of $_n$ is either $0$ or a positive number.\n",
    "      \n",
    "    - A nonzero value for $_n$ implies that $_n$ is a support vector.\n",
    " \n",
    "* Why?\n",
    "\n",
    "    - In the SVM optimization problem, $_n$ represents Lagrange multipliers associated with the constraints. The Lagrange multipliers are non-negative.\n",
    "\n",
    "    - Support vectors are the data points that have non-zero Lagrange multipliers. These are the points that lie on the margin or inside the margin, contributing to the definition of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf5525-31bc-4cf3-ba4d-40072e80e909",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46afc5e-3a49-4a5a-bb4e-f9701a881753",
   "metadata": {},
   "source": [
    "**13. Shane loves candy, but he is allergic to chocolate. Therefore, he cannot eat candy that contains chocolate. A bowl of candy is kept in his classroom. He knows that is contains one of the two types of candy: chocolate or milk. Without taking a bite, there is no way to tell whether a candy is milk or chocolate. However, having devoured candy for years, sweet-toothed Shane has internalized a way to differentiate between these candy types just by looking at its size and shape. Shane's technique can be expressed in terms of conditional probabilities in the following way.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8e1e3-e0b6-467e-83e8-6a892000d95b",
   "metadata": {},
   "source": [
    "| P(shape \\| candy type) | shape=oval | shape=circle |\n",
    "| --- | --- | --- |\n",
    "| candy type = milk | 0.6 | 0.4 |\n",
    "| candy type = chocolate | 0.7 | 0.3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c00ba-0788-413b-875e-82bad7e64085",
   "metadata": {},
   "source": [
    "| P(size \\| candy type) | size=small | size=normal |\n",
    "| --- | --- | --- |\n",
    "| candy type = milk | 0.8 | 0.2 |\n",
    "| candy type = chocolate | 0.5 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262e7e4-0100-4a12-8970-d7ca43cfef28",
   "metadata": {},
   "source": [
    "**Assuming Shane's method of differentiating these candy types following a naive Bayes model, how would we classify a candy that is circular in shape and normal in size?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9ff35-82c8-46ca-8e10-627f69990ddc",
   "metadata": {},
   "source": [
    "**_Naive Bayes_**\n",
    "\n",
    "* Naive Bayes is a conditional probability model:\n",
    "\n",
    "    $P(C_k|\\boldsymbol{X}) = \\frac{P(\\boldsymbol{X}|C_k)P(C_k)}{P(\\boldsymbol{X})}$\n",
    "\n",
    "* Using Bayesian probability terminology:\n",
    "\n",
    "    $posterior = \\frac{likelihood \\times prior}{evidence}$\n",
    "\n",
    "* In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on class\n",
    "\n",
    "    $P(C_k|\\boldsymbol{X}) \\propto P(\\boldsymbol{X}|C_k)P(C_k)$\n",
    "\n",
    "* This is equivalent to the joint probability:\n",
    "\n",
    "    $P(C_k|\\boldsymbol{X}) \\propto P(\\boldsymbol{X} \\cap C_k)$\n",
    "\n",
    "    $P(C_k|\\boldsymbol{X}) \\propto P(C_k \\cap x_1 \\cap x_2 \\cap x_3...x_n)$\n",
    "\n",
    "* This can be rewritten via the chain rule:\n",
    "\n",
    "    $P(C_k|\\boldsymbol{X}) \\propto P(x_1 | x_2,...,x_n, C_k)P(x_2 | x_3,...,x_n, C_k)P(x_{n-1} | x_n, C_k)P(x_{n} | C_k)$\n",
    "\n",
    "* Assuming conditional independence:\n",
    "\n",
    "    $ P(x_i | x_{i+1},...,x_n, C_k) = P(x_i | C_k) $\n",
    "\n",
    "* Enables simplification:\n",
    "\n",
    "    $P(C_k| x_1,...,x_n) \\propto p(C_k)p(x_1 | C_k)p(x_2 | C_k)...p(x_n | C_k)$\n",
    "\n",
    "    $P(C_k| x_1,...,x_n) \\propto p(C_k)\\prod_{i=1}^n p(x_i | C_k)$\n",
    "\n",
    "* Therefore we need to find the class with the maximum probability according to:\n",
    "\n",
    "    $C_k = \\text{argmax}_C \\; \\; \\; p(C_k)\\prod_{i=1}^n p(x_i | C_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753ab01-b2d5-489a-a51c-b104d8dbe0c4",
   "metadata": {},
   "source": [
    "**_Set-up:_**\n",
    "\n",
    "* $C_1$: Chocolate candy\n",
    "\n",
    "* $C_2$: Milk candy\n",
    "\n",
    "* Assume the same number of candies are in the box:\n",
    "\n",
    "    $p(C_1) = p(C_2) = 0.5$\n",
    "\n",
    "* $x_1$: circular shape\n",
    "\n",
    "    $p(x_1 | C_1) = 0.3$\n",
    "\n",
    "    $p(x_1 | C_2) = 0.4$\n",
    "\n",
    "* $x_2$: normal size\n",
    "\n",
    "    $p(x_2 | C_1) = 0.5$\n",
    "\n",
    "    $p(x_2 | C_2) = 0.2$\n",
    "\n",
    "**_Solution_**\n",
    "\n",
    "* Chocolate candy:\n",
    "\n",
    "    $P(C_1| x_1, x_2) \\propto p(C_1) \\times p(x_1 | C_1) \\times p(x_2 | C_1)$\n",
    "    \n",
    "    $P(C_1| x_1, x_2) = 0.5 \\times 0.3 \\times 0.5 = 0.075$\n",
    "\n",
    "* Milk candy:\n",
    "\n",
    "    $P(C_2| x_1, x_2) \\propto p(C_2) \\times p(x_1 | C_2) \\times p(x_2 | C_2)$\n",
    "    \n",
    "    $P(C_2| x_1, x_2) = 0.5 \\times 0.4 \\times 0.2 = 0.04$\n",
    "\n",
    "* Therefore classify as chocolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a6183-0419-4e23-8816-aca6c88d87c9",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2vis",
   "language": "python",
   "name": "nl2vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
